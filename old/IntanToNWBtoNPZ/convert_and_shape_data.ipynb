{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22c1249-44b2-4afb-8685-61ea9f2b8ad5",
   "metadata": {},
   "source": [
    "# Print contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c99f9f7d-f8ba-4c4a-b37d-7cf27f64f0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquisition Groups:\n",
      " - ElectricalSeries\n",
      "\n",
      "Stimulus Groups:\n",
      " - TimeSeries_amp_settle\n",
      " - TimeSeries_charge_recovery\n",
      " - TimeSeries_compliance_limit\n",
      " - TimeSeries_stimulation\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "def list_nwb_components(nwb_file_path):\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        print(\"Acquisition Groups:\")\n",
    "        acquisition = nwbfile.acquisition\n",
    "        for name in acquisition:\n",
    "            print(f\" - {name}\")\n",
    "\n",
    "        if hasattr(nwbfile, 'stimulus'):\n",
    "            print(\"\\nStimulus Groups:\")\n",
    "            stimulus = nwbfile.stimulus\n",
    "            for name in stimulus:\n",
    "                print(f\" - {name}\")\n",
    "\n",
    "# Path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# List all components\n",
    "list_nwb_components(nwb_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a300dca3-9b37-461b-ad1a-9014a9a17fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquisition Groups:\n",
      "\n",
      "ElectricalSeries:\n",
      " - Comments: voltage data recorded from the amplifiers of an Intan Technologies chip\n",
      " - Description: voltage data recorded from the amplifiers of an Intan Technologies chip\n",
      " - Unit: volts\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "Stimulus Groups:\n",
      "\n",
      "TimeSeries_amp_settle:\n",
      " - Comments: amplifier settle activity of an Intan Technologies chip\n",
      " - Description: amplifier settle activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_charge_recovery:\n",
      " - Comments: charge recovery activity of an Intan Technologies chip\n",
      " - Description: charge recovery activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_compliance_limit:\n",
      " - Comments: compliance limit activity of an Intan Technologies chip\n",
      " - Description: compliance limit activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_stimulation:\n",
      " - Comments: current stimulation activity of an Intan Technologies chip\n",
      " - Description: current stimulation activity of an Intan Technologies chip\n",
      " - Unit: amps\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# Replace with the path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# Function to print details of a TimeSeries object\n",
    "def print_timeseries_details(name, timeseries):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\" - Comments: {timeseries.comments}\")\n",
    "    print(f\" - Description: {timeseries.description}\")\n",
    "    print(f\" - Unit: {timeseries.unit}\")\n",
    "    print(f\" - Data shape: {timeseries.data.shape}\")\n",
    "    print(f\" - Timestamps shape: {timeseries.timestamps.shape if timeseries.timestamps else 'No timestamps'}\")\n",
    "\n",
    "# Open the .nwb file using PyNWB\n",
    "with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "\n",
    "    # Access and print details of the acquisition group\n",
    "    print(\"Acquisition Groups:\")\n",
    "    acquisition = nwbfile.acquisition\n",
    "    for name, timeseries in acquisition.items():\n",
    "        print_timeseries_details(name, timeseries)\n",
    "\n",
    "    # Access and print details of the stimulus group\n",
    "    if hasattr(nwbfile, 'stimulus'):\n",
    "        print(\"\\nStimulus Groups:\")\n",
    "        for name, timeseries in nwbfile.stimulus.items():\n",
    "            print_timeseries_details(name, timeseries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "920ebc99-c61e-41e0-bd7c-7426d6ad9442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeSeries_stimulation does not contain any non-zero values.\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np  # Import NumPy\n",
    "\n",
    "# Replace with the path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# Function to check for non-zero values in a TimeSeries component's data\n",
    "def check_for_non_zeros_in_timeseries(name, timeseries):\n",
    "    # Convert the data to a NumPy array\n",
    "    data_array = np.array(timeseries.data)\n",
    "    \n",
    "    # Count non-zero values\n",
    "    non_zero_count = (data_array != 0).sum()\n",
    "    \n",
    "    if non_zero_count > 0:\n",
    "        print(f\"{name} contains {non_zero_count} non-zero values.\")\n",
    "    else:\n",
    "        print(f\"{name} does not contain any non-zero values.\")\n",
    "\n",
    "# Open the .nwb file using PyNWB\n",
    "with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "\n",
    "    # Access and check for non-zero values in the \"TimeSeries_stimulation\" component\n",
    "    if 'TimeSeries_stimulation' in nwbfile.stimulus:\n",
    "        timeseries_stimulation = nwbfile.stimulus['TimeSeries_stimulation']\n",
    "        check_for_non_zeros_in_timeseries(\"TimeSeries_stimulation\", timeseries_stimulation)\n",
    "    else:\n",
    "        print(\"TimeSeries_stimulation not found in the dataset.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e12254-d214-4c48-8ade-fcdf0ecba78e",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77fcda67-a40f-421e-be46-11d10179a727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectricalSeries dataset shape: (65280, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np\n",
    "\n",
    "def extract_electrical_series_data(nwb_file_path):\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        seq_len = 65280  # Number of time points\n",
    "        electrodes = 64  # Total number of electrodes (8x8 grid)\n",
    "\n",
    "        # Extract data from the ElectricalSeries component\n",
    "        electrical_series = nwbfile.acquisition.get('ElectricalSeries')\n",
    "        if electrical_series is None:\n",
    "            raise ValueError(\"ElectricalSeries component not found.\")\n",
    "\n",
    "        data = np.array(electrical_series.data[:])\n",
    "        \n",
    "        # Check the shape of the data\n",
    "        if data.ndim != 2 or data.shape[1] != electrodes:\n",
    "            raise ValueError(\"Incorrect data shape in ElectricalSeries\")\n",
    "\n",
    "        # Reshape the data to (seq_len, 8, 8)\n",
    "        reshaped_data = data.reshape((seq_len, 8, 8))\n",
    "        return reshaped_data\n",
    "\n",
    "# Path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# Extract and reshape ElectricalSeries data\n",
    "electrical_series_data = extract_electrical_series_data(nwb_file_path)\n",
    "print(\"ElectricalSeries dataset shape:\", electrical_series_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121340d-dac4-4e19-b91a-d7310ee5d802",
   "metadata": {},
   "source": [
    "### Print first 10 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1edb02b-372e-42af-ba4e-ae13b5fc202e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 rows of ElectricalSeries:\n",
      "Timepoint 1:\n",
      "[[ 2.53500002e-06  7.21500010e-06 -3.89999997e-07  3.90000014e-06\n",
      "   4.29000011e-06 -7.01999988e-06 -1.13099995e-05  2.73000001e-06]\n",
      " [ 3.50999994e-06  1.40399998e-05 -7.01999988e-06  1.36500000e-06\n",
      "   3.11999997e-06 -2.14500005e-06  1.95000007e-06  9.75000034e-07]\n",
      " [ 2.53500002e-06  7.21500010e-06 -3.89999997e-07  3.90000014e-06\n",
      "   4.29000011e-06 -7.01999988e-06 -1.13099995e-05  2.73000001e-06]\n",
      " [ 3.50999994e-06  1.40399998e-05 -7.01999988e-06  1.36500000e-06\n",
      "   3.11999997e-06 -2.14500005e-06  1.95000007e-06  9.75000034e-07]\n",
      " [ 2.53500002e-06  7.21500010e-06 -3.89999997e-07  3.90000014e-06\n",
      "   4.29000011e-06 -7.01999988e-06 -1.13099995e-05  2.73000001e-06]\n",
      " [ 3.50999994e-06  1.40399998e-05 -7.01999988e-06  1.36500000e-06\n",
      "   3.11999997e-06 -2.14500005e-06  1.95000007e-06  9.75000034e-07]\n",
      " [ 2.53500002e-06  7.21500010e-06 -3.89999997e-07  3.90000014e-06\n",
      "   4.29000011e-06 -7.01999988e-06 -1.13099995e-05  2.73000001e-06]\n",
      " [ 3.50999994e-06  1.40399998e-05 -7.01999988e-06  1.36500000e-06\n",
      "   3.11999997e-06 -2.14500005e-06  1.95000007e-06  9.75000034e-07]]\n",
      "Timepoint 2:\n",
      "[[-5.0700000e-06  5.4600000e-06 -2.9250000e-06 -5.6549998e-06\n",
      "   4.6800001e-06 -9.9449999e-06 -9.7500003e-07  4.6800001e-06]\n",
      " [-9.9449999e-06  2.7300000e-06  5.8500001e-07  2.7300000e-06\n",
      "  -5.2649998e-06  1.2480000e-05 -1.5600000e-06  8.3850000e-06]\n",
      " [-5.0700000e-06  5.4600000e-06 -2.9250000e-06 -5.6549998e-06\n",
      "   4.6800001e-06 -9.9449999e-06 -9.7500003e-07  4.6800001e-06]\n",
      " [-9.9449999e-06  2.7300000e-06  5.8500001e-07  2.7300000e-06\n",
      "  -5.2649998e-06  1.2480000e-05 -1.5600000e-06  8.3850000e-06]\n",
      " [-5.0700000e-06  5.4600000e-06 -2.9250000e-06 -5.6549998e-06\n",
      "   4.6800001e-06 -9.9449999e-06 -9.7500003e-07  4.6800001e-06]\n",
      " [-9.9449999e-06  2.7300000e-06  5.8500001e-07  2.7300000e-06\n",
      "  -5.2649998e-06  1.2480000e-05 -1.5600000e-06  8.3850000e-06]\n",
      " [-5.0700000e-06  5.4600000e-06 -2.9250000e-06 -5.6549998e-06\n",
      "   4.6800001e-06 -9.9449999e-06 -9.7500003e-07  4.6800001e-06]\n",
      " [-9.9449999e-06  2.7300000e-06  5.8500001e-07  2.7300000e-06\n",
      "  -5.2649998e-06  1.2480000e-05 -1.5600000e-06  8.3850000e-06]]\n",
      "Timepoint 3:\n",
      "[[ 1.5600000e-06  5.6549998e-06  1.1505000e-05 -3.3150000e-06\n",
      "   2.1450001e-06  5.8500000e-06  5.4600000e-06 -2.9250000e-06]\n",
      " [-3.7049999e-06  2.7300000e-06 -5.8500001e-07 -2.7300000e-06\n",
      "  -3.9000001e-06  8.1899998e-06 -1.7550000e-06  9.5550004e-06]\n",
      " [ 1.5600000e-06  5.6549998e-06  1.1505000e-05 -3.3150000e-06\n",
      "   2.1450001e-06  5.8500000e-06  5.4600000e-06 -2.9250000e-06]\n",
      " [-3.7049999e-06  2.7300000e-06 -5.8500001e-07 -2.7300000e-06\n",
      "  -3.9000001e-06  8.1899998e-06 -1.7550000e-06  9.5550004e-06]\n",
      " [ 1.5600000e-06  5.6549998e-06  1.1505000e-05 -3.3150000e-06\n",
      "   2.1450001e-06  5.8500000e-06  5.4600000e-06 -2.9250000e-06]\n",
      " [-3.7049999e-06  2.7300000e-06 -5.8500001e-07 -2.7300000e-06\n",
      "  -3.9000001e-06  8.1899998e-06 -1.7550000e-06  9.5550004e-06]\n",
      " [ 1.5600000e-06  5.6549998e-06  1.1505000e-05 -3.3150000e-06\n",
      "   2.1450001e-06  5.8500000e-06  5.4600000e-06 -2.9250000e-06]\n",
      " [-3.7049999e-06  2.7300000e-06 -5.8500001e-07 -2.7300000e-06\n",
      "  -3.9000001e-06  8.1899998e-06 -1.7550000e-06  9.5550004e-06]]\n",
      "Timepoint 4:\n",
      "[[ 5.84999998e-06 -1.75499997e-06  4.48499986e-06  1.94999998e-07\n",
      "  -3.31499996e-06  1.17000002e-06 -3.70499993e-06  1.36500000e-06]\n",
      " [ 2.53500002e-06 -9.75000034e-07 -1.46249995e-05  1.94999998e-07\n",
      "   2.73000001e-06  7.40999985e-06 -9.16499994e-06 -2.53500002e-06]\n",
      " [ 5.84999998e-06 -1.75499997e-06  4.48499986e-06  1.94999998e-07\n",
      "  -3.31499996e-06  1.17000002e-06 -3.70499993e-06  1.36500000e-06]\n",
      " [ 2.53500002e-06 -9.75000034e-07 -1.46249995e-05  1.94999998e-07\n",
      "   2.73000001e-06  7.40999985e-06 -9.16499994e-06 -2.53500002e-06]\n",
      " [ 5.84999998e-06 -1.75499997e-06  4.48499986e-06  1.94999998e-07\n",
      "  -3.31499996e-06  1.17000002e-06 -3.70499993e-06  1.36500000e-06]\n",
      " [ 2.53500002e-06 -9.75000034e-07 -1.46249995e-05  1.94999998e-07\n",
      "   2.73000001e-06  7.40999985e-06 -9.16499994e-06 -2.53500002e-06]\n",
      " [ 5.84999998e-06 -1.75499997e-06  4.48499986e-06  1.94999998e-07\n",
      "  -3.31499996e-06  1.17000002e-06 -3.70499993e-06  1.36500000e-06]\n",
      " [ 2.53500002e-06 -9.75000034e-07 -1.46249995e-05  1.94999998e-07\n",
      "   2.73000001e-06  7.40999985e-06 -9.16499994e-06 -2.53500002e-06]]\n",
      "Timepoint 5:\n",
      "[[-8.970e-06  2.145e-06  5.850e-06 -2.925e-06  6.240e-06 -6.630e-06\n",
      "  -2.925e-06 -7.995e-06]\n",
      " [ 2.925e-06  8.970e-06 -1.950e-07  3.705e-06  1.950e-06 -4.485e-06\n",
      "  -2.730e-06 -6.825e-06]\n",
      " [-8.970e-06  2.145e-06  5.850e-06 -2.925e-06  6.240e-06 -6.630e-06\n",
      "  -2.925e-06 -7.995e-06]\n",
      " [ 2.925e-06  8.970e-06 -1.950e-07  3.705e-06  1.950e-06 -4.485e-06\n",
      "  -2.730e-06 -6.825e-06]\n",
      " [-8.970e-06  2.145e-06  5.850e-06 -2.925e-06  6.240e-06 -6.630e-06\n",
      "  -2.925e-06 -7.995e-06]\n",
      " [ 2.925e-06  8.970e-06 -1.950e-07  3.705e-06  1.950e-06 -4.485e-06\n",
      "  -2.730e-06 -6.825e-06]\n",
      " [-8.970e-06  2.145e-06  5.850e-06 -2.925e-06  6.240e-06 -6.630e-06\n",
      "  -2.925e-06 -7.995e-06]\n",
      " [ 2.925e-06  8.970e-06 -1.950e-07  3.705e-06  1.950e-06 -4.485e-06\n",
      "  -2.730e-06 -6.825e-06]]\n",
      "Timepoint 6:\n",
      "[[-2.535e-06 -6.435e-06 -1.443e-05  7.800e-07  9.750e-07  3.510e-06\n",
      "  -7.410e-06  4.875e-06]\n",
      " [ 7.215e-06  6.240e-06  5.850e-06 -7.995e-06 -1.950e-07  7.605e-06\n",
      "  -1.170e-06 -1.755e-06]\n",
      " [-2.535e-06 -6.435e-06 -1.443e-05  7.800e-07  9.750e-07  3.510e-06\n",
      "  -7.410e-06  4.875e-06]\n",
      " [ 7.215e-06  6.240e-06  5.850e-06 -7.995e-06 -1.950e-07  7.605e-06\n",
      "  -1.170e-06 -1.755e-06]\n",
      " [-2.535e-06 -6.435e-06 -1.443e-05  7.800e-07  9.750e-07  3.510e-06\n",
      "  -7.410e-06  4.875e-06]\n",
      " [ 7.215e-06  6.240e-06  5.850e-06 -7.995e-06 -1.950e-07  7.605e-06\n",
      "  -1.170e-06 -1.755e-06]\n",
      " [-2.535e-06 -6.435e-06 -1.443e-05  7.800e-07  9.750e-07  3.510e-06\n",
      "  -7.410e-06  4.875e-06]\n",
      " [ 7.215e-06  6.240e-06  5.850e-06 -7.995e-06 -1.950e-07  7.605e-06\n",
      "  -1.170e-06 -1.755e-06]]\n",
      "Timepoint 7:\n",
      "[[-4.680e-06  5.460e-06  2.340e-06  2.925e-06  4.875e-06  3.900e-06\n",
      "  -5.850e-06 -1.755e-06]\n",
      " [ 1.560e-06 -9.750e-07  8.580e-06  4.095e-06 -1.950e-06 -7.995e-06\n",
      "  -4.095e-06 -3.900e-06]\n",
      " [-4.680e-06  5.460e-06  2.340e-06  2.925e-06  4.875e-06  3.900e-06\n",
      "  -5.850e-06 -1.755e-06]\n",
      " [ 1.560e-06 -9.750e-07  8.580e-06  4.095e-06 -1.950e-06 -7.995e-06\n",
      "  -4.095e-06 -3.900e-06]\n",
      " [-4.680e-06  5.460e-06  2.340e-06  2.925e-06  4.875e-06  3.900e-06\n",
      "  -5.850e-06 -1.755e-06]\n",
      " [ 1.560e-06 -9.750e-07  8.580e-06  4.095e-06 -1.950e-06 -7.995e-06\n",
      "  -4.095e-06 -3.900e-06]\n",
      " [-4.680e-06  5.460e-06  2.340e-06  2.925e-06  4.875e-06  3.900e-06\n",
      "  -5.850e-06 -1.755e-06]\n",
      " [ 1.560e-06 -9.750e-07  8.580e-06  4.095e-06 -1.950e-06 -7.995e-06\n",
      "  -4.095e-06 -3.900e-06]]\n",
      "Timepoint 8:\n",
      "[[ 0.0000000e+00  0.0000000e+00 -4.2900001e-06  7.9949996e-06\n",
      "   2.5350000e-06 -8.5800002e-06  9.7500003e-07  4.4849999e-06]\n",
      " [ 7.4099999e-06 -7.2150001e-06 -8.7749995e-06 -2.5350000e-06\n",
      "  -9.9449999e-06  9.7500003e-07  1.9500001e-06 -8.7749995e-06]\n",
      " [ 0.0000000e+00  0.0000000e+00 -4.2900001e-06  7.9949996e-06\n",
      "   2.5350000e-06 -8.5800002e-06  9.7500003e-07  4.4849999e-06]\n",
      " [ 7.4099999e-06 -7.2150001e-06 -8.7749995e-06 -2.5350000e-06\n",
      "  -9.9449999e-06  9.7500003e-07  1.9500001e-06 -8.7749995e-06]\n",
      " [ 0.0000000e+00  0.0000000e+00 -4.2900001e-06  7.9949996e-06\n",
      "   2.5350000e-06 -8.5800002e-06  9.7500003e-07  4.4849999e-06]\n",
      " [ 7.4099999e-06 -7.2150001e-06 -8.7749995e-06 -2.5350000e-06\n",
      "  -9.9449999e-06  9.7500003e-07  1.9500001e-06 -8.7749995e-06]\n",
      " [ 0.0000000e+00  0.0000000e+00 -4.2900001e-06  7.9949996e-06\n",
      "   2.5350000e-06 -8.5800002e-06  9.7500003e-07  4.4849999e-06]\n",
      " [ 7.4099999e-06 -7.2150001e-06 -8.7749995e-06 -2.5350000e-06\n",
      "  -9.9449999e-06  9.7500003e-07  1.9500001e-06 -8.7749995e-06]]\n",
      "Timepoint 9:\n",
      "[[ 2.9250000e-06  2.1450001e-06  4.8749998e-06 -2.7300000e-06\n",
      "  -1.9500000e-07  5.4600000e-06 -7.7999999e-07 -6.2399999e-06]\n",
      " [ 7.4099999e-06 -9.5550004e-06  6.6299999e-06  1.3650000e-06\n",
      "   3.9000000e-07  7.7999999e-07  5.6549998e-06  1.9500000e-07]\n",
      " [ 2.9250000e-06  2.1450001e-06  4.8749998e-06 -2.7300000e-06\n",
      "  -1.9500000e-07  5.4600000e-06 -7.7999999e-07 -6.2399999e-06]\n",
      " [ 7.4099999e-06 -9.5550004e-06  6.6299999e-06  1.3650000e-06\n",
      "   3.9000000e-07  7.7999999e-07  5.6549998e-06  1.9500000e-07]\n",
      " [ 2.9250000e-06  2.1450001e-06  4.8749998e-06 -2.7300000e-06\n",
      "  -1.9500000e-07  5.4600000e-06 -7.7999999e-07 -6.2399999e-06]\n",
      " [ 7.4099999e-06 -9.5550004e-06  6.6299999e-06  1.3650000e-06\n",
      "   3.9000000e-07  7.7999999e-07  5.6549998e-06  1.9500000e-07]\n",
      " [ 2.9250000e-06  2.1450001e-06  4.8749998e-06 -2.7300000e-06\n",
      "  -1.9500000e-07  5.4600000e-06 -7.7999999e-07 -6.2399999e-06]\n",
      " [ 7.4099999e-06 -9.5550004e-06  6.6299999e-06  1.3650000e-06\n",
      "   3.9000000e-07  7.7999999e-07  5.6549998e-06  1.9500000e-07]]\n",
      "Timepoint 10:\n",
      "[[-1.170e-06 -7.605e-06  5.850e-06  7.800e-07 -3.900e-07  7.800e-07\n",
      "   5.460e-06  1.365e-06]\n",
      " [ 2.535e-06 -8.580e-06 -2.535e-06  1.092e-05  7.410e-06  3.900e-06\n",
      "  -1.170e-06  7.800e-07]\n",
      " [-1.170e-06 -7.605e-06  5.850e-06  7.800e-07 -3.900e-07  7.800e-07\n",
      "   5.460e-06  1.365e-06]\n",
      " [ 2.535e-06 -8.580e-06 -2.535e-06  1.092e-05  7.410e-06  3.900e-06\n",
      "  -1.170e-06  7.800e-07]\n",
      " [-1.170e-06 -7.605e-06  5.850e-06  7.800e-07 -3.900e-07  7.800e-07\n",
      "   5.460e-06  1.365e-06]\n",
      " [ 2.535e-06 -8.580e-06 -2.535e-06  1.092e-05  7.410e-06  3.900e-06\n",
      "  -1.170e-06  7.800e-07]\n",
      " [-1.170e-06 -7.605e-06  5.850e-06  7.800e-07 -3.900e-07  7.800e-07\n",
      "   5.460e-06  1.365e-06]\n",
      " [ 2.535e-06 -8.580e-06 -2.535e-06  1.092e-05  7.410e-06  3.900e-06\n",
      "  -1.170e-06  7.800e-07]]\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np\n",
    "\n",
    "def print_first_10_rows_electrical_series(nwb_file_path):\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        # Access ElectricalSeries data\n",
    "        electrical_series = nwbfile.acquisition.get('ElectricalSeries')\n",
    "        if electrical_series is None:\n",
    "            print(\"ElectricalSeries component not found.\")\n",
    "            return\n",
    "\n",
    "        # Extract and reshape data\n",
    "        data = np.array(electrical_series.data[:])\n",
    "        seq_len, electrodes = data.shape\n",
    "        reshaped_data = data.reshape((seq_len, 8, 8))\n",
    "\n",
    "        # Print the first 10 rows\n",
    "        print(\"\\nFirst 10 rows of ElectricalSeries:\")\n",
    "        for row in range(10):\n",
    "            print(f\"Timepoint {row + 1}:\\n{reshaped_data[row, :, :]}\")\n",
    "\n",
    "# Path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'  # Replace with the correct file path\n",
    "\n",
    "# Print the first 10 rows of ElectricalSeries\n",
    "print_first_10_rows_electrical_series(nwb_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00df2914-2eaa-425b-87cd-581e8f922937",
   "metadata": {},
   "source": [
    "### Check for non zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25db32f6-c942-4bef-854a-4dce3ce0ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElectricalSeries:\n",
      " - Total non-zero elements: 4173640\n",
      " - Total elements: 4177920\n",
      " - Percentage of non-zero elements: 99.90%\n",
      "\n",
      "TimeSeries_amp_settle:\n",
      " - Total non-zero elements: 0\n",
      " - Total elements: 4177920\n",
      " - Percentage of non-zero elements: 0.00%\n",
      "\n",
      "TimeSeries_charge_recovery:\n",
      " - Total non-zero elements: 0\n",
      " - Total elements: 4177920\n",
      " - Percentage of non-zero elements: 0.00%\n",
      "\n",
      "TimeSeries_compliance_limit:\n",
      " - Total non-zero elements: 0\n",
      " - Total elements: 4177920\n",
      " - Percentage of non-zero elements: 0.00%\n",
      "\n",
      "TimeSeries_stimulation:\n",
      " - Total non-zero elements: 0\n",
      " - Total elements: 4177920\n",
      " - Percentage of non-zero elements: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_for_nonzero_values(datasets):\n",
    "    for component, data in datasets.items():\n",
    "        non_zero_count = np.count_nonzero(data)\n",
    "        total_elements = data.size\n",
    "        print(f\"{component}:\")\n",
    "        print(f\" - Total non-zero elements: {non_zero_count}\")\n",
    "        print(f\" - Total elements: {total_elements}\")\n",
    "        print(f\" - Percentage of non-zero elements: {100 * non_zero_count / total_elements:.2f}%\\n\")\n",
    "\n",
    "# Call the function to check for non-zero values\n",
    "check_for_nonzero_values(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c613e6-3468-4fc3-8247-362154d1f52a",
   "metadata": {},
   "source": [
    "# Next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13a1ee69-6162-47b6-b0a2-b25d02f65c8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic_Intelligence_Labs/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/ElectricalSeries.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Replace 'your/directory/path' with the actual path where your .npz files are located\u001b[39;00m\n\u001b[1;32m    124\u001b[0m directory_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic_Intelligence_Labs/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/ElectricalSeries.npz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 125\u001b[0m \u001b[43mread_in_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 117\u001b[0m, in \u001b[0;36mread_in_data\u001b[0;34m(dirPath)\u001b[0m\n\u001b[1;32m    115\u001b[0m offset_trigger_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    116\u001b[0m listFiles \u001b[38;5;241m=\u001b[39m getListFiles(dirPath)\n\u001b[0;32m--> 117\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset_trigger_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlistFiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverting spatial position\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m dataset \u001b[38;5;241m=\u001b[39m convertFlatRaw4x3x4x3(dataset)\n",
      "Cell \u001b[0;32mIn[29], line 34\u001b[0m, in \u001b[0;36mcreate_array\u001b[0;34m(dirPath, offset, listFiles)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# number of pre and post stimulation files \u001b[39;00m\n\u001b[1;32m     33\u001b[0m nbr_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(listFiles)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m total_nbr_stim_per_file,nbr_stim_per_electrode,nbr_electrodes,nbr_neurospheres,seq_len \u001b[38;5;241m=\u001b[39m \u001b[43mrecording_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# create two arrays of size (N,8,8,3001), each one corresponding to one of the class\u001b[39;00m\n\u001b[1;32m     37\u001b[0m raw1_reshaped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((total_nbr_stim_per_file\u001b[38;5;241m*\u001b[39mnbr_files,nbr_electrodes,nbr_electrodes,seq_len\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[29], line 87\u001b[0m, in \u001b[0;36mrecording_parameters\u001b[0;34m(dirPath, offset)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mINPUT:\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mdirPath: [str] path to directory\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03mseq_len: [int] length of of the data in the time dimension (3000)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m#file_data = load_raw_data(f'{dirPath}/exp_{0}_0_{offset}.npz')\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m file_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_raw_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirPath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m nbr_stim_per_electrode \u001b[38;5;241m=\u001b[39m file_data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m nbr_electrodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m, in \u001b[0;36mload_raw_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_raw_data\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Load the npz file\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Extract the data for each timepoint and store it in a list\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     data \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic_Intelligence_Labs/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/ElectricalSeries.npz'"
     ]
    }
   ],
   "source": [
    "\n",
    "#Reads In Data to (N, 1, 8, 8, S) array\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_raw_data(filename: str):\n",
    "    # Load the npz file\n",
    "    loaded = np.load(filename)\n",
    "    \n",
    "    # Extract the data for each timepoint and store it in a list\n",
    "    data = []\n",
    "    for i in range(4177920):  # Adjust this number based on the actual number of timepoints in your data\n",
    "        timepoint_key = f'timepoint_{i}'  # Construct the key name\n",
    "        if timepoint_key in loaded:\n",
    "            timepoint_data = loaded[timepoint_key]\n",
    "            data.append(timepoint_data)\n",
    "        else:\n",
    "            break  # Exit the loop if the key is not found\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "\n",
    "def create_array(dirPath:str, offset:int,listFiles:list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before before recording after a stimulus\n",
    "    OUTPUT: \n",
    "    dataset: [np.array] an array of shape (N,8,8,3001) containing the data\n",
    "    \"\"\"\n",
    "    # number of pre and post stimulation files \n",
    "    nbr_files = int(len(listFiles)/2)\n",
    "    total_nbr_stim_per_file,nbr_stim_per_electrode,nbr_electrodes,nbr_neurospheres,seq_len = recording_parameters(dirPath,offset)\n",
    "\n",
    "    # create two arrays of size (N,8,8,3001), each one corresponding to one of the class\n",
    "    raw1_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    raw2_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "\n",
    "    # fill these arrays with corresponding values from files\n",
    "    for start_exp_index in range(nbr_files):\n",
    "        print(f'experiment number:{start_exp_index}')\n",
    "        raw1 = load_raw_data(f'{dirPath}/exp_{start_exp_index}_0_{offset}.npz')\n",
    "        raw2 = load_raw_data(f'{dirPath}/exp_{start_exp_index}_1_{offset}.npz')\n",
    "\n",
    "        # reshape (80,8,8,3000)\n",
    "        raw1_one_file = np.zeros((total_nbr_stim_per_file,nbr_electrodes,nbr_electrodes,raw1[0].shape[2]))\n",
    "        raw2_one_file = np.zeros((total_nbr_stim_per_file,nbr_electrodes,nbr_electrodes,raw2[0].shape[2]))\n",
    "\n",
    "        # iterate through electrode stimulated and neurospheres\n",
    "        for electrode in range(nbr_electrodes):\n",
    "            #nbr_neurospheres = int(raw1[electrode].shape[1]/8)\n",
    "            # N: number of reptition of the stimulus\n",
    "            N = raw1[1].shape[0]\n",
    "            for i in range(nbr_neurospheres):\n",
    "                j = nbr_electrodes*i\n",
    "                raw1_one_file[N*i:N*(i+1),electrode] = raw1[electrode][:N,j:j+nbr_electrodes]\n",
    "                raw2_one_file[N*i:N*(i+1),electrode] = raw2[electrode][:N,j:j+nbr_electrodes]\n",
    "        raw1_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw1_one_file\n",
    "        raw2_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw2_one_file\n",
    "\n",
    "    # append label\n",
    "    print(\"append label\")\n",
    "    raw1_reshaped[:,:,:,seq_len] = np.zeros((raw1_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "    raw2_reshaped[:,:,:,seq_len] = np.ones((raw2_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "\n",
    "    #return full dataset\n",
    "    print(\"return dataset\")\n",
    "    dataset = np.zeros((total_nbr_stim_per_file*nbr_files*2,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    dataset[:total_nbr_stim_per_file*nbr_files] = raw1_reshaped\n",
    "    dataset[total_nbr_stim_per_file*nbr_files:] = raw2_reshaped\n",
    "    return dataset.astype(np.float32)\n",
    "\n",
    "def recording_parameters(dirPath:str,offset:int):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before recording after a stimulus\n",
    "    OUTPUT:\n",
    "    total_nbr_stim_per_file: [int] number of electrodes time the number of stimulation per electrode\n",
    "    nbr_stim_per_electrode: [int] number of time the experiment is repeated throughout a file (9)\n",
    "    nbr_electrodes: [int] always 8. \n",
    "    nbr_neurospheres: [int] number of neurospheres considered (4 or 8)\n",
    "    seq_len: [int] length of of the data in the time dimension (3000)\n",
    "    \"\"\"\n",
    "    #file_data = load_raw_data(f'{dirPath}/exp_{0}_0_{offset}.npz')\n",
    "    file_data = load_raw_data(dirPath)\n",
    "    nbr_stim_per_electrode = file_data[1].shape[0]\n",
    "    nbr_electrodes = 8\n",
    "    nbr_neurospheres = int(file_data[1].shape[1]/8)\n",
    "    total_nbr_stim_per_file = nbr_stim_per_electrode*nbr_neurospheres\n",
    "    seq_len = file_data[1].shape[2]\n",
    "    return total_nbr_stim_per_file,nbr_stim_per_electrode,nbr_electrodes,nbr_neurospheres,seq_len\n",
    "\n",
    "def convertFlatRaw4x3x4x3(raw: np.array):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    raw: [np.array] array of shape (N,1,8,8,M) with N the number of trials and M the length of the sequence\n",
    "    OUTPUT:\n",
    "    array4x3: [np.array] array of shape (N,1,4,3,4,3,M)\n",
    "    \"\"\"\n",
    "    map8 = np.array([[1,0],[0,1],[1,1],[1,2],[2,2],[2,1],[3,1],[2,0]])\n",
    "    array4x3 = torch.zeros((raw.shape[0],1,4,3,4,3,raw.shape[-1]),dtype=torch.float32)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            array4x3[:,0,map8[i,0],map8[i,1],map8[j,0],map8[j,1],:] = raw[:,0,i,j,:]\n",
    "    return array4x3\n",
    "\n",
    "def getListFiles(dirPath):\n",
    "    listFiles = filter(os.path.isfile,glob.glob(f'{dirPath}/*.npz'))\n",
    "    listFiles = sorted(listFiles, key=os.path.getmtime)\n",
    "    return listFiles\n",
    "\n",
    "def read_in_data(dirPath):\n",
    "    offset_trigger_ms = 5\n",
    "    listFiles = getListFiles(dirPath)\n",
    "    dataset = create_array(dirPath, offset_trigger_ms, listFiles)\n",
    "    print(\"converting spatial position\")\n",
    "    dataset = convertFlatRaw4x3x4x3(dataset)\n",
    "    dataset = np.expand_dims(dataset,1)\n",
    "    return dataset\n",
    "\n",
    "# Replace 'your/directory/path' with the actual path where your .npz files are located\n",
    "directory_path = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic_Intelligence_Labs/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/ElectricalSeries.npz'\n",
    "read_in_data(directory_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d48da8-745b-4393-9440-abefbe9a7bc6",
   "metadata": {},
   "source": [
    "# Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7138dbe-0d2e-4f5c-8b44-6176507baa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final dataset: (21, 3000, 64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the original dataset\n",
    "original_dataset = np.load('ElectricalSeries.npz')['data']\n",
    "\n",
    "# Define the desired length of each dataset\n",
    "desired_length = 3000\n",
    "\n",
    "# Calculate the number of datasets\n",
    "num_datasets = len(original_dataset) // desired_length\n",
    "\n",
    "# Calculate the number of remaining datapoints after division\n",
    "remaining_datapoints = len(original_dataset) % desired_length\n",
    "\n",
    "# Initialize an empty list to store the reshaped datasets\n",
    "reshaped_datasets = []\n",
    "\n",
    "# Reshape the original dataset into N datasets of shape (64, 3000)\n",
    "for i in range(num_datasets):\n",
    "    start_idx = i * desired_length\n",
    "    end_idx = start_idx + desired_length\n",
    "    reshaped_datasets.append(original_dataset[start_idx:end_idx])\n",
    "\n",
    "# If there are remaining datapoints, you can choose to remove the last incomplete dataset\n",
    "if remaining_datapoints >= desired_length:\n",
    "    start_idx = num_datasets * desired_length\n",
    "    end_idx = start_idx + desired_length\n",
    "    reshaped_datasets.append(original_dataset[start_idx:end_idx])\n",
    "\n",
    "# Convert the list of reshaped datasets into a single numpy array\n",
    "final_dataset = np.array(reshaped_datasets)\n",
    "\n",
    "# Print the shape of the final dataset\n",
    "print(\"Shape of the final dataset:\", final_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe966bbc-c076-40a3-9887-f3849febf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# convert (N,1,8,8,M) to (N,1,4,3,4,3,M)\n",
    "def convertFlatRaw4x3x4x3(raw: np.array):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    raw: [np.array] array of shape (N,1,8,8,M) with N the number of trials and M the length of the sequence\n",
    "    OUTPUT:\n",
    "    array4x3: [np.array] array of shape (N,1,4,3,4,3,M)\n",
    "    \"\"\"\n",
    "    map8 = np.array([[1,0],[0,1],[1,1],[1,2],[2,2],[2,1],[3,1],[2,0]])\n",
    "    array4x3 = torch.zeros((raw.shape[0],1,4,3,4,3,raw.shape[-1]),dtype=torch.float32)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            array4x3[:,0,map8[i,0],map8[i,1],map8[j,0],map8[j,1],:] = raw[:,0,i,j,:]\n",
    "    return array4x3\n",
    "\n",
    "# compute important parameters to create the dataset\n",
    "def recording_parameters(dirPath:str,offset:int):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before recording after a stimulus\n",
    "    OUTPUT:\n",
    "    total_nbr_stim_per_file: [int] number of electrodes time the number of stimulation per electrode\n",
    "    nbr_stim_per_electrode: [int] number of time the experiment is repeated throughout a file (9)\n",
    "    nbr_electrodes: [int] always 8. \n",
    "    nbr_neurospheres: [int] number of neurospheres considered (4 or 8)\n",
    "    seq_len: [int] length of of the data in the time dimension (3000)\n",
    "    \"\"\"\n",
    "    file_data = load_raw_data(f'{dirPath}/exp_{0}_0_{offset}.npz')\n",
    "    nbr_stim_per_electrode = file_data[1].shape[0]\n",
    "    nbr_electrodes = 8\n",
    "    nbr_neurospheres = int(file_data[1].shape[1]/8)\n",
    "    total_nbr_stim_per_file = nbr_stim_per_electrode*nbr_neurospheres\n",
    "    seq_len = file_data[1].shape[2]\n",
    "    return total_nbr_stim_per_file,nbr_stim_per_electrode,nbr_electrodes,nbr_neurospheres,seq_len\n",
    "\n",
    "# create an array of shape (N,8,8,3000)\n",
    "def create_array(dirPath:str, offset:int,listFiles:list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before before recording after a stimulus\n",
    "    OUTPUT: \n",
    "    dataset: [np.array] an array of shape (N,8,8,3001) containing the data\n",
    "    \"\"\"\n",
    "    # number of pre and post stimulation files \n",
    "    nbr_files = int(len(listFiles)/2)\n",
    "    total_nbr_stim_per_file,nbr_stim_per_electrode,nbr_electrodes,nbr_neurospheres,seq_len = recording_parameters(dirPath,offset)\n",
    "\n",
    "\n",
    "    # create two arrays of size (N,8,8,3001), each one corresponding to one of the class\n",
    "    raw1_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    raw2_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "\n",
    "    # fill these arrays with corresponding values from files\n",
    "    for start_exp_index in range(nbr_files):\n",
    "        print(f'experiment number:{start_exp_index}')\n",
    "        raw1 = load_raw_data(f'{dirPath}/exp_{start_exp_index}_0_{offset}.npz')\n",
    "        raw2 = load_raw_data(f'{dirPath}/exp_{start_exp_index}_1_{offset}.npz')\n",
    "\n",
    "        # reshape (80,8,8,3000)\n",
    "        raw1_one_file = np.zeros((total_nbr_stim_per_file,nbr_electrodes,nbr_electrodes,raw1[0].shape[2]))\n",
    "        raw2_one_file = np.zeros((total_nbr_stim_per_file,nbr_electrodes,nbr_electrodes,raw2[0].shape[2]))\n",
    "\n",
    "        # iterate through electrode stimulated and neurospheres\n",
    "        for electrode in range(nbr_electrodes):\n",
    "            #nbr_neurospheres = int(raw1[electrode].shape[1]/8)\n",
    "            # N: number of reptition of the stimulus\n",
    "            N = raw1[1].shape[0]\n",
    "            for i in range(nbr_neurospheres):\n",
    "                j = nbr_electrodes*i\n",
    "                raw1_one_file[N*i:N*(i+1),electrode] = raw1[electrode][:N,j:j+nbr_electrodes]\n",
    "                raw2_one_file[N*i:N*(i+1),electrode] = raw2[electrode][:N,j:j+nbr_electrodes]\n",
    "        raw1_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw1_one_file\n",
    "        raw2_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw2_one_file\n",
    "\n",
    "    # append label\n",
    "    print(\"append label\")\n",
    "    raw1_reshaped[:,:,:,seq_len] = np.zeros((raw1_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "    raw2_reshaped[:,:,:,seq_len] = np.ones((raw2_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "\n",
    "    #return full dataset\n",
    "    print(\"return dataset\")\n",
    "    dataset = np.zeros((total_nbr_stim_per_file*nbr_files*2,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    dataset[:total_nbr_stim_per_file*nbr_files] = raw1_reshaped\n",
    "    dataset[total_nbr_stim_per_file*nbr_files:] = raw2_reshaped\n",
    "    return dataset.astype(np.float32)\n",
    "\n",
    "\n",
    "# reduce the dimension of the recordings using wavelet transforms (3000->750)\n",
    "def wavelet_filter(data:np.array):\n",
    "    w1 = data[:,:,:,:,:-1]\n",
    "    (w1,_)=pywt.dwt(w1,wavelet='db4',axis=-1,mode='per')\n",
    "    (w1,_)=pywt.dwt(w1,wavelet='db4',axis=-1,mode='per')\n",
    "    return np.concatenate((w1,data[:,:,:,:,[-1]]),axis=-1)\n",
    "     \n",
    "\n",
    "offset_ms = 5\n",
    "dataset = create_array(dirPath, offset_ms,listFiles)\n",
    "dataset = np.expand_dims(dataset,1)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "rng.shuffle(dataset,axis=0)\n",
    "dataset = wavelet_filter(dataset).astype(np.float32)\n",
    "print(dataset.shape)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b6934-6bb2-4b11-b16f-ddaa97de174a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
