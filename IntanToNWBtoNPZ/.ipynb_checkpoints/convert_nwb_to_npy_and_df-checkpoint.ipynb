{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361ddcf7-48a7-405b-9e51-4c0b1fc7aaf8",
   "metadata": {},
   "source": [
    "# Reshape the data for a single spheroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a230fc4-f78a-4424-a110-e55589105cb3",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddd15b-96e9-4921-9069-b46efa613a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881c829-bf8c-4f82-babb-aeb62d35aed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21014be-7f5f-43cb-9785-eea2dabe8cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e69c20-7b7e-449d-8740-07ea2079949c",
   "metadata": {},
   "source": [
    "# Extract nwb file contents and save as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbe7cf35-bc99-421d-bb1e-bc0938310a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and saving completed.\n",
      "Data extraction and saving completed.\n",
      "experiment number: 0\n",
      "append label\n",
      "experiment number: 1\n",
      "append label\n",
      "experiment number: 2\n",
      "append label\n",
      "experiment number: 3\n",
      "append label\n",
      "experiment number: 4\n",
      "append label\n",
      "experiment number: 5\n",
      "append label\n",
      "experiment number: 6\n",
      "append label\n",
      "experiment number: 7\n",
      "append label\n",
      "experiment number: 8\n",
      "append label\n",
      "experiment number: 9\n",
      "append label\n",
      "experiment number: 10\n",
      "append label\n",
      "experiment number: 11\n",
      "append label\n",
      "experiment number: 12\n",
      "append label\n",
      "experiment number: 13\n",
      "append label\n",
      "experiment number: 14\n",
      "append label\n",
      "experiment number: 15\n",
      "append label\n",
      "experiment number: 16\n",
      "append label\n",
      "experiment number: 17\n",
      "append label\n",
      "experiment number: 18\n",
      "append label\n",
      "experiment number: 19\n",
      "append label\n",
      "experiment number: 20\n",
      "append label\n",
      "experiment number: 21\n",
      "append label\n",
      "experiment number: 22\n",
      "append label\n",
      "experiment number: 23\n",
      "append label\n",
      "experiment number: 24\n",
      "append label\n",
      "experiment number: 25\n",
      "append label\n",
      "experiment number: 26\n",
      "append label\n",
      "experiment number: 27\n",
      "append label\n",
      "experiment number: 28\n",
      "append label\n",
      "experiment number: 29\n",
      "append label\n",
      "experiment number: 30\n",
      "append label\n",
      "experiment number: 31\n",
      "append label\n",
      "experiment number: 32\n",
      "append label\n",
      "experiment number: 33\n",
      "append label\n",
      "experiment number: 34\n",
      "append label\n",
      "experiment number: 35\n",
      "append label\n",
      "experiment number: 36\n",
      "append label\n",
      "experiment number: 37\n",
      "append label\n",
      "experiment number: 38\n",
      "append label\n",
      "experiment number: 39\n",
      "append label\n",
      "experiment number: 40\n",
      "append label\n",
      "experiment number: 41\n",
      "append label\n",
      "experiment number: 42\n",
      "append label\n",
      "experiment number: 43\n",
      "append label\n",
      "experiment number: 44\n",
      "append label\n",
      "experiment number: 45\n",
      "append label\n",
      "experiment number: 46\n",
      "append label\n",
      "experiment number: 47\n",
      "append label\n",
      "experiment number: 48\n",
      "append label\n",
      "experiment number: 49\n",
      "append label\n",
      "experiment number: 50\n",
      "append label\n",
      "experiment number: 51\n",
      "append label\n",
      "experiment number: 52\n",
      "append label\n",
      "experiment number: 53\n",
      "append label\n",
      "experiment number: 54\n",
      "append label\n",
      "experiment number: 55\n",
      "append label\n",
      "experiment number: 56\n",
      "append label\n",
      "experiment number: 57\n",
      "append label\n",
      "experiment number: 58\n",
      "append label\n",
      "experiment number: 59\n",
      "append label\n",
      "experiment number: 60\n",
      "append label\n",
      "experiment number: 61\n",
      "append label\n",
      "experiment number: 62\n",
      "append label\n",
      "experiment number: 63\n",
      "append label\n",
      "return dataset\n",
      "Dataset shape before converting: (8355840, 1, 8, 8, 2)\n",
      "converting spatial position\n",
      "Dataset shape before converting: (8355840, 1, 1, 4, 3, 4, 3, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 182\u001b[0m\n\u001b[1;32m    180\u001b[0m dataset \u001b[38;5;241m=\u001b[39m read_in_data(output_dir)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape before converting:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should now be a 5D array\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m converted_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mconvertFlatRaw4x3x4x3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverted dataset shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, converted_dataset\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should now be the correct 7D array\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 150\u001b[0m, in \u001b[0;36mconvertFlatRaw4x3x4x3\u001b[0;34m(raw)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mConvert a 5D numpy array of shape (N, 1, 8, 8, 2) to a 7D numpy array of shape (N, 1, 4, 3, 4, 3, 2).\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    149\u001b[0m map8 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m--> 150\u001b[0m N, _, _, _, last_dim \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# Correctly adjusted to match the shape of the input array\u001b[39;00m\n\u001b[1;32m    151\u001b[0m array4x3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, last_dim))\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch as torch\n",
    "\n",
    "# Define the function to load raw data from .npz files\n",
    "def load_raw_data(filename: str):\n",
    "    with np.load(filename) as loaded:\n",
    "        # Assuming the .npz file contains an array with the key 'data'\n",
    "        data = loaded['data']\n",
    "    return data\n",
    "    \n",
    "# Function to extract data from a TimeSeries object\n",
    "def extract_data(time_series):\n",
    "    return {\n",
    "        'data': np.array(time_series.data[:]),\n",
    "        'timestamps': np.array(time_series.timestamps[:]) if time_series.timestamps else None,\n",
    "        'unit': time_series.unit,\n",
    "        'comments': time_series.comments\n",
    "    }\n",
    "\n",
    "# Function to extract and save data from NWB file as NPZ\n",
    "def extract_and_save_npz(nwb_file_path, output_dir, offset=5):  # Added offset parameter with default value\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        # Extract and save acquisition and stimulus data as NPZ\n",
    "        for group_name in ('acquisition', 'stimulus'):\n",
    "            if hasattr(nwbfile, group_name):\n",
    "                group_data = getattr(nwbfile, group_name)\n",
    "                for name, timeseries in group_data.items():\n",
    "                    data = extract_data(timeseries)['data']\n",
    "                    \n",
    "                    # Generate file names based on your naming convention\n",
    "                    for i in range(data.shape[1]):\n",
    "                        for group_type in [0, 1]:  # Assuming you have two types of groups\n",
    "                            file_name = f'exp_{i}_{group_type}_{offset}.npz'  # offset is now defined\n",
    "                            # Generate and save data for each file\n",
    "                            np.savez_compressed(os.path.join(output_dir, file_name), data=data[:, i])\n",
    "\n",
    "    print(\"Data extraction and saving completed.\")\n",
    "\n",
    "# Replace with the path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# Directory to save the extracted data\n",
    "output_dir = 'ready_for_shaping'  # Adjust this path as needed\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Call the function to extract data from .nwb and save as .npz\n",
    "extract_and_save_npz(nwb_file_path, output_dir)\n",
    "\n",
    "def create_array(dirPath:str, offset:int,listFiles:list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before before recording after a stimulus\n",
    "    OUTPUT: \n",
    "    dataset: [np.array] an array of shape (N,8,8,3001) containing the data\n",
    "    \"\"\"\n",
    "    # number of pre and post stimulation files \n",
    "    nbr_files = int(len(listFiles) / 2)\n",
    "    total_nbr_stim_per_file, nbr_stim_per_electrode, nbr_electrodes, nbr_neurospheres, seq_len = recording_parameters(dirPath, offset)\n",
    "\n",
    "    # Initialize two arrays, each corresponding to one of the class\n",
    "    raw1_reshaped = np.zeros((total_nbr_stim_per_file * nbr_files, nbr_electrodes, nbr_electrodes, seq_len + 1))\n",
    "    raw2_reshaped = np.zeros((total_nbr_stim_per_file * nbr_files, nbr_electrodes, nbr_electrodes, seq_len + 1))\n",
    "\n",
    "    for start_exp_index in range(nbr_files):\n",
    "        print(f'experiment number: {start_exp_index}')\n",
    "        \n",
    "        filename1 = f'{dirPath}/exp_{start_exp_index}_0_{offset}.npz'\n",
    "        filename2 = f'{dirPath}/exp_{start_exp_index}_1_{offset}.npz'\n",
    "\n",
    "        if not os.path.exists(filename1) or not os.path.exists(filename2):\n",
    "            print(f\"Files not found: {filename1} or {filename2}\")\n",
    "            continue\n",
    "\n",
    "        raw1 = load_raw_data(filename1)\n",
    "        raw2 = load_raw_data(filename2)\n",
    "\n",
    "        # Append label\n",
    "        print(\"append label\")\n",
    "        raw1_reshaped[:, :, :, seq_len] = np.zeros((raw1_reshaped.shape[0], nbr_electrodes, nbr_electrodes))\n",
    "        raw2_reshaped[:, :, :, seq_len] = np.ones((raw2_reshaped.shape[0], nbr_electrodes, nbr_electrodes))\n",
    "\n",
    "    # Return full dataset in 5D format\n",
    "    print(\"return dataset\")\n",
    "    dataset = np.zeros((total_nbr_stim_per_file * nbr_files * 2, 1, nbr_electrodes, nbr_electrodes, seq_len + 1))\n",
    "    dataset[:total_nbr_stim_per_file * nbr_files, 0, :, :, :] = raw1_reshaped\n",
    "    dataset[total_nbr_stim_per_file * nbr_files:, 0, :, :, :] = raw2_reshaped\n",
    "\n",
    "    print(\"Dataset shape before converting:\", dataset.shape)  # Should be (N, 1, 8, 8, M)\n",
    "\n",
    "    return dataset.astype(np.float32)\n",
    "\n",
    "\n",
    "def recording_parameters(dirPath: str, offset: int):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory containing .npz files\n",
    "    offset: [int] time delay before recording after a stimulus (unused in this function)\n",
    "    OUTPUT:\n",
    "    total_nbr_stim_per_file: [int] total number of stimulations per file\n",
    "    nbr_stim_per_electrode: [int] number of times the experiment is repeated within a file\n",
    "    nbr_electrodes: [int] number of electrodes (usually 8)\n",
    "    nbr_neurospheres: [int] number of neurospheres considered\n",
    "    seq_len: [int] length of the data in the time dimension\n",
    "    \"\"\"\n",
    "\n",
    "    npz_files = [f for f in os.listdir(dirPath) if f.endswith('.npz')]\n",
    "    if not npz_files:\n",
    "        raise FileNotFoundError(\"No .npz files found in the directory\")\n",
    "\n",
    "    first_file = os.path.join(dirPath, npz_files[0])\n",
    "    file_data = load_raw_data(first_file)\n",
    "\n",
    "    if not isinstance(file_data, np.ndarray):\n",
    "        raise ValueError(\"Loaded data is not a numpy array.\")\n",
    "\n",
    "    # Adjust the logic to handle different data shapes\n",
    "    if file_data.ndim == 1:\n",
    "        # Data is 1-dimensional\n",
    "        nbr_trials = len(file_data)\n",
    "        time_points = 1  # Assuming each point is a separate trial\n",
    "    elif file_data.ndim == 2:\n",
    "        # Data is 2-dimensional\n",
    "        nbr_trials = file_data.shape[0]\n",
    "        time_points = file_data.shape[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of dimensions in the data: {file_data.ndim}\")\n",
    "\n",
    "    # Assuming 8 electrodes, 1 neurosphere, and 1 stimulation per electrode\n",
    "    nbr_stim_per_electrode = 1\n",
    "    nbr_electrodes = 8\n",
    "    nbr_neurospheres = 1\n",
    "    total_nbr_stim_per_file = nbr_trials\n",
    "    seq_len = time_points\n",
    "\n",
    "    return total_nbr_stim_per_file, nbr_stim_per_electrode, nbr_electrodes, nbr_neurospheres, seq_len\n",
    "\n",
    "def convertFlatRaw4x3x4x3(raw: np.array):\n",
    "    \"\"\"\n",
    "    Convert a 5D numpy array of shape (N, 1, 8, 8, 2) to a 7D numpy array of shape (N, 1, 4, 3, 4, 3, 2).\n",
    "    \"\"\"\n",
    "    map8 = np.array([[1, 0], [0, 1], [1, 1], [1, 2], [2, 2], [2, 1], [3, 1], [2, 0]])\n",
    "    N, _, _, _, last_dim = raw.shape  # Correctly adjusted to match the shape of the input array\n",
    "    array4x3 = np.zeros((N, 1, 4, 3, 4, 3, last_dim))\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            array4x3[:, 0, map8[i, 0], map8[i, 1], map8[j, 0], map8[j, 1], :] = raw[:, 0, i, j, :]\n",
    "\n",
    "    return array4x3\n",
    "\n",
    "def getListFiles(dirPath):\n",
    "    listFiles = filter(os.path.isfile,glob.glob(f'{dirPath}/*.npz'))\n",
    "    listFiles = sorted(listFiles, key=os.path.getmtime)\n",
    "    return listFiles\n",
    "        \n",
    "def read_in_data(dirPath):\n",
    "    offset_trigger_ms = 5\n",
    "    listFiles = getListFiles(dirPath)\n",
    "    dataset = create_array(dirPath, offset_trigger_ms, listFiles)\n",
    "    print(\"converting spatial position\")\n",
    "    dataset = convertFlatRaw4x3x4x3(dataset)\n",
    "    dataset = np.expand_dims(dataset,1)\n",
    "    return dataset\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Call the function to extract data from .nwb and save as .npz\n",
    "extract_and_save_npz(nwb_file_path, output_dir, offset=5)  # Pass the offset value here\n",
    "\n",
    "# After creating the dataset\n",
    "dataset = read_in_data(output_dir)\n",
    "print(\"Dataset shape before converting:\", dataset.shape)  # Should now be a 5D array\n",
    "converted_dataset = convertFlatRaw4x3x4x3(dataset)\n",
    "print(\"Converted dataset shape:\", converted_dataset.shape)  # Should now be the correct 7D array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd54697-9ec0-4987-9074-8a4216a3adc5",
   "metadata": {},
   "source": [
    "# Delete all files in ready_for_shaping dir incase of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d7ca6b-145b-46e7-943f-525c92f02fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic_Intelligence_Labs/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/ready_for_shaping'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Iterate through the list of files and delete each one\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c1249-44b2-4afb-8685-61ea9f2b8ad5",
   "metadata": {},
   "source": [
    "# Print contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a300dca3-9b37-461b-ad1a-9014a9a17fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquisition Groups:\n",
      "\n",
      "ElectricalSeries:\n",
      " - Comments: voltage data recorded from the amplifiers of an Intan Technologies chip\n",
      " - Description: voltage data recorded from the amplifiers of an Intan Technologies chip\n",
      " - Unit: volts\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "Stimulus Groups:\n",
      "\n",
      "TimeSeries_amp_settle:\n",
      " - Comments: amplifier settle activity of an Intan Technologies chip\n",
      " - Description: amplifier settle activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_charge_recovery:\n",
      " - Comments: charge recovery activity of an Intan Technologies chip\n",
      " - Description: charge recovery activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_compliance_limit:\n",
      " - Comments: compliance limit activity of an Intan Technologies chip\n",
      " - Description: compliance limit activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_stimulation:\n",
      " - Comments: current stimulation activity of an Intan Technologies chip\n",
      " - Description: current stimulation activity of an Intan Technologies chip\n",
      " - Unit: amps\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# Replace with the path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# Function to print details of a TimeSeries object\n",
    "def print_timeseries_details(name, timeseries):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\" - Comments: {timeseries.comments}\")\n",
    "    print(f\" - Description: {timeseries.description}\")\n",
    "    print(f\" - Unit: {timeseries.unit}\")\n",
    "    print(f\" - Data shape: {timeseries.data.shape}\")\n",
    "    print(f\" - Timestamps shape: {timeseries.timestamps.shape if timeseries.timestamps else 'No timestamps'}\")\n",
    "\n",
    "# Open the .nwb file using PyNWB\n",
    "with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "\n",
    "    # Access and print details of the acquisition group\n",
    "    print(\"Acquisition Groups:\")\n",
    "    acquisition = nwbfile.acquisition\n",
    "    for name, timeseries in acquisition.items():\n",
    "        print_timeseries_details(name, timeseries)\n",
    "\n",
    "    # Access and print details of the stimulus group\n",
    "    if hasattr(nwbfile, 'stimulus'):\n",
    "        print(\"\\nStimulus Groups:\")\n",
    "        for name, timeseries in nwbfile.stimulus.items():\n",
    "            print_timeseries_details(name, timeseries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3719dedc-3bab-4c2e-ac4f-47fa4b0cfb86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_and_save_npz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Call the function to extract data from .nwb and save as .npz\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[43mextract_and_save_npz\u001b[49m(nwb_file_path, output_dir)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Now call the read_in_data function with the directory containing your .npz files\u001b[39;00m\n\u001b[1;32m    130\u001b[0m dataset \u001b[38;5;241m=\u001b[39m read_in_data(output_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_and_save_npz' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79038fb-1be5-498a-84af-d9f99e2c3c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
