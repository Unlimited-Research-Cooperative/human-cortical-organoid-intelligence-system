{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361ddcf7-48a7-405b-9e51-4c0b1fc7aaf8",
   "metadata": {},
   "source": [
    "# Reshape the data for a single spheroid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a230fc4-f78a-4424-a110-e55589105cb3",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddd15b-96e9-4921-9069-b46efa613a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881c829-bf8c-4f82-babb-aeb62d35aed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21014be-7f5f-43cb-9785-eea2dabe8cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e69c20-7b7e-449d-8740-07ea2079949c",
   "metadata": {},
   "source": [
    "# Extract nwb file contents and save as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e12254-d214-4c48-8ade-fcdf0ecba78e",
   "metadata": {},
   "source": [
    "### Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcda67-a40f-421e-be46-11d10179a727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_nwb_data(nwb_file_path: str):\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "        dataset = []\n",
    "\n",
    "        for group_name in ('acquisition', 'stimulus'):\n",
    "            if hasattr(nwbfile, group_name):\n",
    "                group_data = getattr(nwbfile, group_name)\n",
    "                for name, timeseries in group_data.items():\n",
    "                    data = np.array(timeseries.data[:])\n",
    "                    print(f\"Data shape for {name}: {data.shape}\")  # Debugging line\n",
    "\n",
    "                    try:\n",
    "                        # Assuming data is in the format: electrodes x seq_len\n",
    "                        electrodes = int(np.sqrt(data.shape[0]))\n",
    "                        seq_len = data.shape[1]\n",
    "                        reshaped_data = data.reshape((1, electrodes, electrodes, seq_len))\n",
    "\n",
    "                        class_label = np.zeros((1, electrodes, electrodes, 1)) if group_name == 'acquisition' else np.ones((1, electrodes, electrodes, 1))\n",
    "                        combined_data = np.concatenate((reshaped_data, class_label), axis=-1)\n",
    "                        dataset.append(combined_data)\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error reshaping data for {name}: {e}\")\n",
    "\n",
    "        if dataset:\n",
    "            dataset = np.concatenate(dataset, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"No valid data found in NWB file.\")\n",
    "\n",
    "    return dataset.astype(np.float32)\n",
    "\n",
    "\n",
    "# Convert dataset to required 7D format\n",
    "def convert_to_7d_format(dataset: np.array):\n",
    "    \"\"\"\n",
    "    Converts a 5D dataset to a 7D format.\n",
    "\n",
    "    :param dataset: The input dataset of shape (N, 1, 8, 8, M)\n",
    "    :return: Converted dataset of shape (N, 1, 4, 3, 4, 3, M)\n",
    "    \"\"\"\n",
    "    map8 = np.array([[1, 0], [0, 1], [1, 1], [1, 2], [2, 2], [2, 1], [3, 1], [2, 0]])\n",
    "    N, _, _, _, M = dataset.shape\n",
    "    converted_dataset = np.zeros((N, 1, 4, 3, 4, 3, M))\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            converted_dataset[:, 0, map8[i, 0], map8[i, 1], map8[j, 0], map8[j, 1], :] = dataset[:, 0, i, j, :]\n",
    "\n",
    "    return converted_dataset\n",
    "\n",
    "# Main processing function\n",
    "def process_data_from_nwb(nwb_file_path: str):\n",
    "    \"\"\"\n",
    "    Main function to process data from NWB file and convert it to the required format.\n",
    "\n",
    "    :param nwb_file_path: Path to the .nwb file\n",
    "    \"\"\"\n",
    "    dataset = process_nwb_data(nwb_file_path)\n",
    "    print(\"Dataset shape after initial processing:\", dataset.shape)\n",
    "\n",
    "    converted_dataset = convert_to_7d_format(dataset)\n",
    "    print(\"Converted dataset shape:\", converted_dataset.shape)\n",
    "\n",
    "    return converted_dataset\n",
    "\n",
    "# Path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# Process the data\n",
    "final_dataset = process_data_from_nwb('data1.nwb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b228b6-9ce8-4c9b-bb28-c044ed5c963e",
   "metadata": {},
   "source": [
    "### Primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe7cf35-bc99-421d-bb1e-bc0938310a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and saving completed.\n",
      "Data extraction and saving completed.\n",
      "experiment number: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (65280,) into shape (65280,8,8,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 167\u001b[0m\n\u001b[1;32m    164\u001b[0m extract_and_save_npz(nwb_file_path, output_dir, offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Pass the offset value here\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# After creating the dataset\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mread_in_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape before converting:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should now be a 5D array\u001b[39;00m\n\u001b[1;32m    169\u001b[0m converted_dataset \u001b[38;5;241m=\u001b[39m convertFlatRaw4x3x4x3(dataset)\n",
      "Cell \u001b[0;32mIn[8], line 154\u001b[0m, in \u001b[0;36mread_in_data\u001b[0;34m(dirPath)\u001b[0m\n\u001b[1;32m    152\u001b[0m offset_trigger_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    153\u001b[0m listFiles \u001b[38;5;241m=\u001b[39m getListFiles(dirPath)\n\u001b[0;32m--> 154\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset_trigger_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlistFiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverting spatial position\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m dataset \u001b[38;5;241m=\u001b[39m convertFlatRaw4x3x4x3(dataset)\n",
      "Cell \u001b[0;32mIn[8], line 83\u001b[0m, in \u001b[0;36mcreate_array\u001b[0;34m(dirPath, offset, listFiles)\u001b[0m\n\u001b[1;32m     81\u001b[0m         end_index \u001b[38;5;241m=\u001b[39m (start_exp_index \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m group_type \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m total_nbr_stim_per_file\n\u001b[1;32m     82\u001b[0m         start_index \u001b[38;5;241m=\u001b[39m end_index \u001b[38;5;241m-\u001b[39m total_nbr_stim_per_file\n\u001b[0;32m---> 83\u001b[0m         \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m raw_data\n\u001b[1;32m     84\u001b[0m         dataset[start_index:end_index, :, :, seq_len] \u001b[38;5;241m=\u001b[39m label\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (65280,) into shape (65280,8,8,1)"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch as torch\n",
    "\n",
    "# Define the function to load raw data from .npz files\n",
    "def load_raw_data(filename: str):\n",
    "    with np.load(filename) as loaded:\n",
    "        # Assuming the .npz file contains an array with the key 'data'\n",
    "        data = loaded['data']\n",
    "    return data\n",
    "    \n",
    "# Function to extract data from a TimeSeries object\n",
    "def extract_data(time_series):\n",
    "    return {\n",
    "        'data': np.array(time_series.data[:]),\n",
    "        'timestamps': np.array(time_series.timestamps[:]) if time_series.timestamps else None,\n",
    "        'unit': time_series.unit,\n",
    "        'comments': time_series.comments\n",
    "    }\n",
    "\n",
    "# Function to extract and save data from NWB file as NPZ\n",
    "def extract_and_save_npz(nwb_file_path, output_dir, offset=5):  # Added offset parameter with default value\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        # Extract and save acquisition and stimulus data as NPZ\n",
    "        for group_name in ('acquisition', 'stimulus'):\n",
    "            if hasattr(nwbfile, group_name):\n",
    "                group_data = getattr(nwbfile, group_name)\n",
    "                for name, timeseries in group_data.items():\n",
    "                    data = extract_data(timeseries)['data']\n",
    "                    \n",
    "                    # Generate file names based on your naming convention\n",
    "                    for i in range(data.shape[1]):\n",
    "                        for group_type in [0, 1]:  # Assuming you have two types of groups\n",
    "                            file_name = f'exp_{i}_{group_type}_{offset}.npz'  # offset is now defined\n",
    "                            # Generate and save data for each file\n",
    "                            np.savez_compressed(os.path.join(output_dir, file_name), data=data[:, i])\n",
    "\n",
    "    print(\"Data extraction and saving completed.\")\n",
    "\n",
    "# Replace with the path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# Directory to save the extracted data\n",
    "output_dir = 'ready_for_shaping'  # Adjust this path as needed\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Call the function to extract data from .nwb and save as .npz\n",
    "extract_and_save_npz(nwb_file_path, output_dir)\n",
    "\n",
    "def create_array(dirPath: str, offset: int, listFiles: list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before recording after a stimulus\n",
    "    OUTPUT: \n",
    "    dataset: [np.array] an array of shape (N, 8, 8, 3001) containing the data\n",
    "    \"\"\"\n",
    "    nbr_files = int(len(listFiles) / 2)\n",
    "    total_nbr_stim_per_file, nbr_stim_per_electrode, nbr_electrodes, nbr_neurospheres, seq_len = recording_parameters(dirPath, offset)\n",
    "\n",
    "    # Initialize an array to store all data\n",
    "    dataset = np.zeros((total_nbr_stim_per_file * nbr_files * 2, nbr_electrodes, nbr_electrodes, seq_len + 1))\n",
    "\n",
    "    for start_exp_index in range(nbr_files):\n",
    "        print(f'experiment number: {start_exp_index}')\n",
    "        for group_type in [0, 1]:  # Loop over group types\n",
    "            filename = f'{dirPath}/exp_{start_exp_index}_{group_type}_{offset}.npz'\n",
    "            if not os.path.exists(filename):\n",
    "                print(f\"File not found: {filename}\")\n",
    "                continue\n",
    "\n",
    "            raw_data = load_raw_data(filename)\n",
    "            # Set the label based on group type and append to the dataset\n",
    "            label = group_type\n",
    "            end_index = (start_exp_index * 2 + group_type + 1) * total_nbr_stim_per_file\n",
    "            start_index = end_index - total_nbr_stim_per_file\n",
    "            dataset[start_index:end_index, :, :, :seq_len] = raw_data\n",
    "            dataset[start_index:end_index, :, :, seq_len] = label\n",
    "\n",
    "    return dataset.astype(np.float32)\n",
    "\n",
    "def recording_parameters(dirPath: str, offset: int):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory containing .npz files\n",
    "    offset: [int] time delay before recording after a stimulus (unused in this function)\n",
    "    OUTPUT:\n",
    "    total_nbr_stim_per_file: [int] total number of stimulations per file\n",
    "    nbr_stim_per_electrode: [int] number of times the experiment is repeated within a file\n",
    "    nbr_electrodes: [int] number of electrodes (usually 8)\n",
    "    nbr_neurospheres: [int] number of neurospheres considered\n",
    "    seq_len: [int] length of the data in the time dimension\n",
    "    \"\"\"\n",
    "\n",
    "    npz_files = [f for f in os.listdir(dirPath) if f.endswith('.npz')]\n",
    "    if not npz_files:\n",
    "        raise FileNotFoundError(\"No .npz files found in the directory\")\n",
    "\n",
    "    first_file = os.path.join(dirPath, npz_files[0])\n",
    "    file_data = load_raw_data(first_file)\n",
    "\n",
    "    if not isinstance(file_data, np.ndarray):\n",
    "        raise ValueError(\"Loaded data is not a numpy array.\")\n",
    "\n",
    "    # Adjust the logic to handle different data shapes\n",
    "    if file_data.ndim == 1:\n",
    "        # Data is 1-dimensional\n",
    "        nbr_trials = len(file_data)\n",
    "        time_points = 1  # Assuming each point is a separate trial\n",
    "    elif file_data.ndim == 2:\n",
    "        # Data is 2-dimensional\n",
    "        nbr_trials = file_data.shape[0]\n",
    "        time_points = file_data.shape[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of dimensions in the data: {file_data.ndim}\")\n",
    "\n",
    "    # Assuming 8 electrodes, 1 neurosphere, and 1 stimulation per electrode\n",
    "    nbr_stim_per_electrode = 1\n",
    "    nbr_electrodes = 8\n",
    "    nbr_neurospheres = 1\n",
    "    total_nbr_stim_per_file = nbr_trials\n",
    "    seq_len = time_points\n",
    "\n",
    "    return total_nbr_stim_per_file, nbr_stim_per_electrode, nbr_electrodes, nbr_neurospheres, seq_len\n",
    "\n",
    "def convertFlatRaw4x3x4x3(raw: np.array):\n",
    "    \"\"\"\n",
    "    Convert a 5D numpy array of shape (N, 1, 8, 8, 2) to a 7D numpy array of shape (N, 1, 4, 3, 4, 3, 2).\n",
    "    \"\"\"\n",
    "    map8 = np.array([[1, 0], [0, 1], [1, 1], [1, 2], [2, 2], [2, 1], [3, 1], [2, 0]])\n",
    "    N, _, _, _, last_dim = raw.shape  # Correctly adjusted to match the shape of the input array\n",
    "    array4x3 = np.zeros((N, 1, 4, 3, 4, 3, last_dim))\n",
    "\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            array4x3[:, 0, map8[i, 0], map8[i, 1], map8[j, 0], map8[j, 1], :] = raw[:, 0, i, j, :]\n",
    "\n",
    "    return array4x3\n",
    "\n",
    "def getListFiles(dirPath):\n",
    "    listFiles = filter(os.path.isfile,glob.glob(f'{dirPath}/*.npz'))\n",
    "    listFiles = sorted(listFiles, key=os.path.getmtime)\n",
    "    return listFiles\n",
    "        \n",
    "def read_in_data(dirPath):\n",
    "    offset_trigger_ms = 5\n",
    "    listFiles = getListFiles(dirPath)\n",
    "    dataset = create_array(dirPath, offset_trigger_ms, listFiles)\n",
    "    print(\"converting spatial position\")\n",
    "    dataset = convertFlatRaw4x3x4x3(dataset)\n",
    "    dataset = np.expand_dims(dataset,1)\n",
    "    return dataset\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Call the function to extract data from .nwb and save as .npz\n",
    "extract_and_save_npz(nwb_file_path, output_dir, offset=5)  # Pass the offset value here\n",
    "\n",
    "# After creating the dataset\n",
    "dataset = read_in_data(output_dir)\n",
    "print(\"Dataset shape before converting:\", dataset.shape)  # Should now be a 5D array\n",
    "converted_dataset = convertFlatRaw4x3x4x3(dataset)\n",
    "print(\"Converted dataset shape:\", converted_dataset.shape)  # Should now be the correct 7D array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd54697-9ec0-4987-9074-8a4216a3adc5",
   "metadata": {},
   "source": [
    "# Delete all files in ready_for_shaping dir incase of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9d7ca6b-145b-46e7-943f-525c92f02fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic_Intelligence_Labs/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/ready_for_shaping'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "\n",
    "# Iterate through the list of files and delete each one\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory_path, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22c1249-44b2-4afb-8685-61ea9f2b8ad5",
   "metadata": {},
   "source": [
    "# Print contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a300dca3-9b37-461b-ad1a-9014a9a17fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquisition Groups:\n",
      "\n",
      "ElectricalSeries:\n",
      " - Comments: voltage data recorded from the amplifiers of an Intan Technologies chip\n",
      " - Description: voltage data recorded from the amplifiers of an Intan Technologies chip\n",
      " - Unit: volts\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "Stimulus Groups:\n",
      "\n",
      "TimeSeries_amp_settle:\n",
      " - Comments: amplifier settle activity of an Intan Technologies chip\n",
      " - Description: amplifier settle activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_charge_recovery:\n",
      " - Comments: charge recovery activity of an Intan Technologies chip\n",
      " - Description: charge recovery activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_compliance_limit:\n",
      " - Comments: compliance limit activity of an Intan Technologies chip\n",
      " - Description: compliance limit activity of an Intan Technologies chip\n",
      " - Unit: digital event\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n",
      "\n",
      "TimeSeries_stimulation:\n",
      " - Comments: current stimulation activity of an Intan Technologies chip\n",
      " - Description: current stimulation activity of an Intan Technologies chip\n",
      " - Unit: amps\n",
      " - Data shape: (65280, 64)\n",
      " - Timestamps shape: (65280,)\n"
     ]
    }
   ],
   "source": [
    "from pynwb import NWBHDF5IO\n",
    "\n",
    "# Replace with the path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "\n",
    "# Function to print details of a TimeSeries object\n",
    "def print_timeseries_details(name, timeseries):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\" - Comments: {timeseries.comments}\")\n",
    "    print(f\" - Description: {timeseries.description}\")\n",
    "    print(f\" - Unit: {timeseries.unit}\")\n",
    "    print(f\" - Data shape: {timeseries.data.shape}\")\n",
    "    print(f\" - Timestamps shape: {timeseries.timestamps.shape if timeseries.timestamps else 'No timestamps'}\")\n",
    "\n",
    "# Open the .nwb file using PyNWB\n",
    "with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "    nwbfile = io.read()\n",
    "\n",
    "    # Access and print details of the acquisition group\n",
    "    print(\"Acquisition Groups:\")\n",
    "    acquisition = nwbfile.acquisition\n",
    "    for name, timeseries in acquisition.items():\n",
    "        print_timeseries_details(name, timeseries)\n",
    "\n",
    "    # Access and print details of the stimulus group\n",
    "    if hasattr(nwbfile, 'stimulus'):\n",
    "        print(\"\\nStimulus Groups:\")\n",
    "        for name, timeseries in nwbfile.stimulus.items():\n",
    "            print_timeseries_details(name, timeseries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3719dedc-3bab-4c2e-ac4f-47fa4b0cfb86",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_and_save_npz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Call the function to extract data from .nwb and save as .npz\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[43mextract_and_save_npz\u001b[49m(nwb_file_path, output_dir)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Now call the read_in_data function with the directory containing your .npz files\u001b[39;00m\n\u001b[1;32m    130\u001b[0m dataset \u001b[38;5;241m=\u001b[39m read_in_data(output_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_and_save_npz' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79038fb-1be5-498a-84af-d9f99e2c3c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
