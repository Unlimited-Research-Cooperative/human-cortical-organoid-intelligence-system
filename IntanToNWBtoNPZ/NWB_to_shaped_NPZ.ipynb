{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a84c1f-29c6-4464-bdc8-700a71d20466",
   "metadata": {},
   "source": [
    "# NWB to NPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea205260-0a2d-4bdf-82e7-a5cb305d03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and saving completed.\n",
      "experiment number: 0\n",
      "Files not found: /home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/exp_0_0_5.npz or /home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/exp_0_1_5.npz\n",
      "experiment number: 1\n",
      "Files not found: /home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/exp_1_0_5.npz or /home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/exp_1_1_5.npz\n",
      "append label\n",
      "return dataset\n",
      "converting spatial position\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 4-dimensional, but 5 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 161\u001b[0m\n\u001b[1;32m    158\u001b[0m extract_and_save_npz(nwb_file_path, output_dir)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Now call the read_in_data function with the directory containing your .npz files\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mread_in_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 146\u001b[0m, in \u001b[0;36mread_in_data\u001b[0;34m(dirPath)\u001b[0m\n\u001b[1;32m    144\u001b[0m dataset \u001b[38;5;241m=\u001b[39m create_array(dirPath, offset_trigger_ms, listFiles)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverting spatial position\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mconvertFlatRaw4x3x4x3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m dataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(dataset,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[3], line 133\u001b[0m, in \u001b[0;36mconvertFlatRaw4x3x4x3\u001b[0;34m(raw)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m         array4x3[:,\u001b[38;5;241m0\u001b[39m,map8[i,\u001b[38;5;241m0\u001b[39m],map8[i,\u001b[38;5;241m1\u001b[39m],map8[j,\u001b[38;5;241m0\u001b[39m],map8[j,\u001b[38;5;241m1\u001b[39m],:] \u001b[38;5;241m=\u001b[39m \u001b[43mraw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array4x3\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 4-dimensional, but 5 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob  \n",
    "from pynwb import NWBHDF5IO\n",
    "import torch\n",
    "\n",
    "def extract_and_save_npz(nwb_file_path, output_dir):\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        # Extract acquisition groups\n",
    "        acquisition_data = {name: np.array(timeseries.data[:]) for name, timeseries in nwbfile.acquisition.items()}\n",
    "        \n",
    "        # Extract stimulus groups if present\n",
    "        stimulus_data = {name: np.array(timeseries.data[:]) for name, timeseries in nwbfile.stimulus.items()} if hasattr(nwbfile, 'stimulus') else {}\n",
    "\n",
    "        # Combine and save the data to .npz files\n",
    "        for name, data in {**acquisition_data, **stimulus_data}.items():\n",
    "            np.savez_compressed(os.path.join(output_dir, f'{name}.npz'), data=data)\n",
    "\n",
    "        print(\"Data extraction and saving completed.\")\n",
    "\n",
    "def load_raw_data(filename: str):\n",
    "    with np.load(filename) as loaded:\n",
    "        # Assuming the .npz file contains an array with the key 'data'\n",
    "        data = loaded['data']\n",
    "    return data\n",
    "\n",
    "def create_array(dirPath:str, offset:int,listFiles:list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before before recording after a stimulus\n",
    "    OUTPUT: \n",
    "    dataset: [np.array] an array of shape (N,8,8,3001) containing the data\n",
    "    \"\"\"\n",
    "    # number of pre and post stimulation files \n",
    "    nbr_files = int(len(listFiles)/2)\n",
    "    total_nbr_stim_per_file,nbr_stim_per_electrode,nbr_electrodes,nbr_neurospheres,seq_len = recording_parameters(dirPath,offset)\n",
    "\n",
    "\n",
    "    # create two arrays of size (N,8,8,3001), each one corresponding to one of the class\n",
    "    raw1_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    raw2_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "\n",
    "    # fill these arrays with corresponding values from files\n",
    "    for start_exp_index in range(nbr_files):\n",
    "        print(f'experiment number: {start_exp_index}')\n",
    "        filename1 = f'{dirPath}/exp_{start_exp_index}_0_{offset}.npz'\n",
    "        filename2 = f'{dirPath}/exp_{start_exp_index}_1_{offset}.npz'\n",
    "\n",
    "        if not os.path.exists(filename1) or not os.path.exists(filename2):\n",
    "            print(f\"Files not found: {filename1} or {filename2}\")\n",
    "            continue  # Skip this iteration if files do not exist\n",
    "\n",
    "        raw1 = load_raw_data(filename1)\n",
    "        raw2 = load_raw_data(filename2)\n",
    "\n",
    "        # iterate through electrode stimulated and neurospheres\n",
    "        for electrode in range(nbr_electrodes):\n",
    "            #nbr_neurospheres = int(raw1[electrode].shape[1]/8)\n",
    "            # N: number of reptition of the stimulus\n",
    "            N = raw1[1].shape[0]\n",
    "            for i in range(nbr_neurospheres):\n",
    "                j = nbr_electrodes*i\n",
    "                raw1_one_file[N*i:N*(i+1),electrode] = raw1[electrode][:N,j:j+nbr_electrodes]\n",
    "                raw2_one_file[N*i:N*(i+1),electrode] = raw2[electrode][:N,j:j+nbr_electrodes]\n",
    "        raw1_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw1_one_file\n",
    "        raw2_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw2_one_file\n",
    "\n",
    "    # append label\n",
    "    print(\"append label\")\n",
    "    raw1_reshaped[:,:,:,seq_len] = np.zeros((raw1_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "    raw2_reshaped[:,:,:,seq_len] = np.ones((raw2_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "\n",
    "    #return full dataset\n",
    "    print(\"return dataset\")\n",
    "    dataset = np.zeros((total_nbr_stim_per_file*nbr_files*2,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    dataset[:total_nbr_stim_per_file*nbr_files] = raw1_reshaped\n",
    "    dataset[total_nbr_stim_per_file*nbr_files:] = raw2_reshaped\n",
    "    return dataset.astype(np.float32)\n",
    "\n",
    "def recording_parameters(dirPath: str, offset: int):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory containing .npz files\n",
    "    offset: [int] time delay before recording after a stimulus (unused in this function)\n",
    "    OUTPUT:\n",
    "    total_nbr_stim_per_file: [int] total number of stimulations per file\n",
    "    nbr_stim_per_electrode: [int] number of times the experiment is repeated within a file\n",
    "    nbr_electrodes: [int] number of electrodes (usually 8)\n",
    "    nbr_neurospheres: [int] number of neurospheres considered\n",
    "    seq_len: [int] length of the data in the time dimension\n",
    "    \"\"\"\n",
    "\n",
    "    npz_files = [f for f in os.listdir(dirPath) if f.endswith('.npz')]\n",
    "    if not npz_files:\n",
    "        raise FileNotFoundError(\"No .npz files found in the directory\")\n",
    "\n",
    "    first_file = os.path.join(dirPath, npz_files[0])\n",
    "    file_data = load_raw_data(first_file)\n",
    "\n",
    "    if not isinstance(file_data, np.ndarray):\n",
    "        raise ValueError(\"Loaded data is not a numpy array.\")\n",
    "    if file_data.ndim != 2:\n",
    "        raise ValueError(f\"Unexpected number of dimensions in the data: {file_data.ndim}\")\n",
    "\n",
    "    # Assuming first dimension is trials and second dimension is time points\n",
    "    nbr_trials = file_data.shape[0]\n",
    "    time_points = file_data.shape[1]\n",
    "\n",
    "    # Assuming 8 electrodes, 1 neurosphere, and 1 stimulation per electrode\n",
    "    nbr_stim_per_electrode = 1\n",
    "    nbr_electrodes = 8\n",
    "    nbr_neurospheres = 1\n",
    "    total_nbr_stim_per_file = nbr_trials\n",
    "    seq_len = time_points\n",
    "\n",
    "    return total_nbr_stim_per_file, nbr_stim_per_electrode, nbr_electrodes, nbr_neurospheres, seq_len\n",
    "\n",
    "\n",
    "def convertFlatRaw4x3x4x3(raw: np.array):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    raw: [np.array] array of shape (N,1,8,8,M) with N the number of trials and M the length of the sequence\n",
    "    OUTPUT:\n",
    "    array4x3: [np.array] array of shape (N,1,4,3,4,3,M)\n",
    "    \"\"\"\n",
    "    map8 = np.array([[1,0],[0,1],[1,1],[1,2],[2,2],[2,1],[3,1],[2,0]])\n",
    "    array4x3 = torch.zeros((raw.shape[0],1,4,3,4,3,raw.shape[-1]),dtype=torch.float32)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            array4x3[:,0,map8[i,0],map8[i,1],map8[j,0],map8[j,1],:] = raw[:,0,i,j,:]\n",
    "    return array4x3\n",
    "\n",
    "def getListFiles(dirPath):\n",
    "    listFiles = filter(os.path.isfile,glob.glob(f'{dirPath}/*.npz'))\n",
    "    listFiles = sorted(listFiles, key=os.path.getmtime)\n",
    "    return listFiles\n",
    "        \n",
    "def read_in_data(dirPath):\n",
    "    offset_trigger_ms = 5\n",
    "    listFiles = getListFiles(dirPath)\n",
    "    dataset = create_array(dirPath, offset_trigger_ms, listFiles)\n",
    "    print(\"converting spatial position\")\n",
    "    dataset = convertFlatRaw4x3x4x3(dataset)\n",
    "    dataset = np.expand_dims(dataset,1)\n",
    "    return dataset\n",
    "\n",
    "# Define the path to your .nwb file and the output directory for .npz files\n",
    "nwb_file_path = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/data1.nwb'  # path to your .nwb file\n",
    "output_dir = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ'  # path to the directory where you want to save .npz files\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Call the function to extract data from .nwb and save as .npz\n",
    "extract_and_save_npz(nwb_file_path, output_dir)\n",
    "\n",
    "# Now call the read_in_data function with the directory containing your .npz files\n",
    "dataset = read_in_data(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856add5-5a5f-42d8-a928-9ee221563305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8f97-4582-49bf-9aa7-880ebd3e51e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
