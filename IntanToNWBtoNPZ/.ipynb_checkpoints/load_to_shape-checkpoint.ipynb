{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a84c1f-29c6-4464-bdc8-700a71d20466",
   "metadata": {},
   "source": [
    "# NWB to NPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea205260-0a2d-4bdf-82e7-a5cb305d03dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and saving completed.\n",
      "experiment number:0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/exp_0_0_5.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 158\u001b[0m\n\u001b[1;32m    155\u001b[0m extract_and_save_npz(nwb_file_path, output_dir)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Now call the read_in_data function with the directory containing your .npz files\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mread_in_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 141\u001b[0m, in \u001b[0;36mread_in_data\u001b[0;34m(dirPath)\u001b[0m\n\u001b[1;32m    139\u001b[0m offset_trigger_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    140\u001b[0m listFiles \u001b[38;5;241m=\u001b[39m getListFiles(dirPath)\n\u001b[0;32m--> 141\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset_trigger_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlistFiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverting spatial position\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m dataset \u001b[38;5;241m=\u001b[39m convertFlatRaw4x3x4x3(dataset)\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mcreate_array\u001b[0;34m(dirPath, offset, listFiles)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_exp_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nbr_files):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment number:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_exp_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m     raw1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_raw_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirPath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/exp_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstart_exp_index\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_0_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moffset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.npz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     raw2 \u001b[38;5;241m=\u001b[39m load_raw_data(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirPath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/exp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_exp_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_1_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# reshape (80,8,8,3000)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36mload_raw_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_raw_data\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m loaded:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Assuming the .npz file contains an array with the key 'data'\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         data \u001b[38;5;241m=\u001b[39m loaded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/exp_0_0_5.npz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob  \n",
    "from pynwb import NWBHDF5IO\n",
    "import torch\n",
    "\n",
    "def extract_and_save_npz(nwb_file_path, output_dir):\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        # Extract acquisition groups\n",
    "        acquisition_data = {name: np.array(timeseries.data[:]) for name, timeseries in nwbfile.acquisition.items()}\n",
    "        \n",
    "        # Extract stimulus groups if present\n",
    "        stimulus_data = {name: np.array(timeseries.data[:]) for name, timeseries in nwbfile.stimulus.items()} if hasattr(nwbfile, 'stimulus') else {}\n",
    "\n",
    "        # Combine and save the data to .npz files\n",
    "        for name, data in {**acquisition_data, **stimulus_data}.items():\n",
    "            np.savez_compressed(os.path.join(output_dir, f'{name}.npz'), data=data)\n",
    "\n",
    "        print(\"Data extraction and saving completed.\")\n",
    "\n",
    "def load_raw_data(filename: str):\n",
    "    with np.load(filename) as loaded:\n",
    "        # Assuming the .npz file contains an array with the key 'data'\n",
    "        data = loaded['data']\n",
    "    return data\n",
    "\n",
    "def create_array(dirPath:str, offset:int,listFiles:list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before before recording after a stimulus\n",
    "    OUTPUT: \n",
    "    dataset: [np.array] an array of shape (N,8,8,3001) containing the data\n",
    "    \"\"\"\n",
    "    # number of pre and post stimulation files \n",
    "    nbr_files = int(len(listFiles)/2)\n",
    "    total_nbr_stim_per_file,nbr_stim_per_electrode,nbr_electrodes,nbr_neurospheres,seq_len = recording_parameters(dirPath,offset)\n",
    "\n",
    "\n",
    "    # create two arrays of size (N,8,8,3001), each one corresponding to one of the class\n",
    "    raw1_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    raw2_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "\n",
    "    # fill these arrays with corresponding values from files\n",
    "    for start_exp_index in range(nbr_files):\n",
    "        print(f'experiment number: {start_exp_index}')\n",
    "        filename1 = f'{dirPath}/exp_{start_exp_index}_0_{offset}.npz'\n",
    "        filename2 = f'{dirPath}/exp_{start_exp_index}_1_{offset}.npz'\n",
    "\n",
    "        if not os.path.exists(filename1) or not os.path.exists(filename2):\n",
    "            print(f\"Files not found: {filename1} or {filename2}\")\n",
    "            continue  # Skip this iteration if files do not exist\n",
    "\n",
    "        raw1 = load_raw_data(filename1)\n",
    "        raw2 = load_raw_data(filename2)\n",
    "\n",
    "        # iterate through electrode stimulated and neurospheres\n",
    "        for electrode in range(nbr_electrodes):\n",
    "            #nbr_neurospheres = int(raw1[electrode].shape[1]/8)\n",
    "            # N: number of reptition of the stimulus\n",
    "            N = raw1[1].shape[0]\n",
    "            for i in range(nbr_neurospheres):\n",
    "                j = nbr_electrodes*i\n",
    "                raw1_one_file[N*i:N*(i+1),electrode] = raw1[electrode][:N,j:j+nbr_electrodes]\n",
    "                raw2_one_file[N*i:N*(i+1),electrode] = raw2[electrode][:N,j:j+nbr_electrodes]\n",
    "        raw1_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw1_one_file\n",
    "        raw2_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw2_one_file\n",
    "\n",
    "    # append label\n",
    "    print(\"append label\")\n",
    "    raw1_reshaped[:,:,:,seq_len] = np.zeros((raw1_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "    raw2_reshaped[:,:,:,seq_len] = np.ones((raw2_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "\n",
    "    #return full dataset\n",
    "    print(\"return dataset\")\n",
    "    dataset = np.zeros((total_nbr_stim_per_file*nbr_files*2,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    dataset[:total_nbr_stim_per_file*nbr_files] = raw1_reshaped\n",
    "    dataset[total_nbr_stim_per_file*nbr_files:] = raw2_reshaped\n",
    "    return dataset.astype(np.float32)\n",
    "\n",
    "def recording_parameters(dirPath: str, offset: int):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory containing .npz files\n",
    "    offset: [int] time delay before recording after a stimulus (unused in this function)\n",
    "    OUTPUT:\n",
    "    total_nbr_stim_per_file: [int] total number of stimulations per file\n",
    "    nbr_stim_per_electrode: [int] number of times the experiment is repeated within a file\n",
    "    nbr_electrodes: [int] number of electrodes (usually 8)\n",
    "    nbr_neurospheres: [int] number of neurospheres considered\n",
    "    seq_len: [int] length of the data in the time dimension\n",
    "    \"\"\"\n",
    "\n",
    "    npz_files = [f for f in os.listdir(dirPath) if f.endswith('.npz')]\n",
    "    if not npz_files:\n",
    "        raise FileNotFoundError(\"No .npz files found in the directory\")\n",
    "\n",
    "    first_file = os.path.join(dirPath, npz_files[0])\n",
    "    file_data = load_raw_data(first_file)\n",
    "\n",
    "    if not isinstance(file_data, np.ndarray):\n",
    "        raise ValueError(\"Loaded data is not a numpy array.\")\n",
    "    if file_data.ndim != 2:\n",
    "        raise ValueError(f\"Unexpected number of dimensions in the data: {file_data.ndim}\")\n",
    "\n",
    "    # Assuming first dimension is trials and second dimension is time points\n",
    "    nbr_trials = file_data.shape[0]\n",
    "    time_points = file_data.shape[1]\n",
    "\n",
    "    # Assuming 8 electrodes, 1 neurosphere, and 1 stimulation per electrode\n",
    "    nbr_stim_per_electrode = 1\n",
    "    nbr_electrodes = 8\n",
    "    nbr_neurospheres = 1\n",
    "    total_nbr_stim_per_file = nbr_trials\n",
    "    seq_len = time_points\n",
    "\n",
    "    return total_nbr_stim_per_file, nbr_stim_per_electrode, nbr_electrodes, nbr_neurospheres, seq_len\n",
    "\n",
    "\n",
    "def convertFlatRaw4x3x4x3(raw: np.array):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    raw: [np.array] array of shape (N,1,8,8,M) with N the number of trials and M the length of the sequence\n",
    "    OUTPUT:\n",
    "    array4x3: [np.array] array of shape (N,1,4,3,4,3,M)\n",
    "    \"\"\"\n",
    "    map8 = np.array([[1,0],[0,1],[1,1],[1,2],[2,2],[2,1],[3,1],[2,0]])\n",
    "    array4x3 = torch.zeros((raw.shape[0],1,4,3,4,3,raw.shape[-1]),dtype=torch.float32)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            array4x3[:,0,map8[i,0],map8[i,1],map8[j,0],map8[j,1],:] = raw[:,0,i,j,:]\n",
    "    return array4x3\n",
    "\n",
    "def getListFiles(dirPath):\n",
    "    listFiles = filter(os.path.isfile,glob.glob(f'{dirPath}/*.npz'))\n",
    "    listFiles = sorted(listFiles, key=os.path.getmtime)\n",
    "    return listFiles\n",
    "        \n",
    "def read_in_data(dirPath):\n",
    "    offset_trigger_ms = 5\n",
    "    listFiles = getListFiles(dirPath)\n",
    "    dataset = create_array(dirPath, offset_trigger_ms, listFiles)\n",
    "    print(\"converting spatial position\")\n",
    "    dataset = convertFlatRaw4x3x4x3(dataset)\n",
    "    dataset = np.expand_dims(dataset,1)\n",
    "    return dataset\n",
    "\n",
    "# Define the path to your .nwb file and the output directory for .npz files\n",
    "nwb_file_path = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/data1.nwb'  # path to your .nwb file\n",
    "output_dir = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ'  # path to the directory where you want to save .npz files\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Call the function to extract data from .nwb and save as .npz\n",
    "extract_and_save_npz(nwb_file_path, output_dir)\n",
    "\n",
    "# Now call the read_in_data function with the directory containing your .npz files\n",
    "dataset = read_in_data(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856add5-5a5f-42d8-a928-9ee221563305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8f97-4582-49bf-9aa7-880ebd3e51e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
