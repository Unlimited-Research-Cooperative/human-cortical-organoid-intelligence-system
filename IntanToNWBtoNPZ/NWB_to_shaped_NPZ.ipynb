{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a84c1f-29c6-4464-bdc8-700a71d20466",
   "metadata": {},
   "source": [
    "# NWB to NPZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea205260-0a2d-4bdf-82e7-a5cb305d03dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "Unable to open file /home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/data1.nwb in 'r' mode. File does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 193\u001b[0m\n\u001b[1;32m    190\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Call the function to extract data from .nwb and save as .npz\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[43mextract_and_save_npz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnwb_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Now call the read_in_data function with the directory containing your .npz files\u001b[39;00m\n\u001b[1;32m    196\u001b[0m dataset \u001b[38;5;241m=\u001b[39m read_in_data(output_dir)\n",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m, in \u001b[0;36mextract_and_save_npz\u001b[0;34m(nwb_file_path, output_dir)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_and_save_npz\u001b[39m(nwb_file_path, output_dir):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mNWBHDF5IO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnwb_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m io:\n\u001b[1;32m     35\u001b[0m         nwbfile \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# Extract acquisition groups\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/hdmf/utils.py:664\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    663\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pynwb/__init__.py:253\u001b[0m, in \u001b[0;36mNWBHDF5IO.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         manager \u001b[38;5;241m=\u001b[39m get_manager()\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Open the file\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmanager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mherd_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mherd_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/hdmf/utils.py:664\u001b[0m, in \u001b[0;36mdocval.<locals>.dec.<locals>.func_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    663\u001b[0m     pargs \u001b[38;5;241m=\u001b[39m _check_args(args, kwargs)\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/hdmf/backends/hdf5/h5tools.py:84\u001b[0m, in \u001b[0;36mHDF5IO.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path) \u001b[38;5;129;01mand\u001b[39;00m (mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m driver \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mros3\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     83\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to open file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m mode. File does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (path, mode)\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedOperation(msg)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path) \u001b[38;5;129;01mand\u001b[39;00m (mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     87\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to open file \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m mode. File already exists.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (path, mode)\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: Unable to open file /home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/data1.nwb in 'r' mode. File does not exist."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pynwb import NWBHDF5IO\n",
    "import torch\n",
    "\n",
    "# Replace with the path to your .nwb file\n",
    "nwb_file_path = 'data1.nwb'\n",
    "output_dir = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ'\n",
    "\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def extract_and_save_npz(nwb_file_path, output_dir):\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        # Extract acquisition and stimulus groups\n",
    "        data_to_save = {}\n",
    "        for group_name in ('acquisition', 'stimulus'):\n",
    "            if hasattr(nwbfile, group_name):\n",
    "                group_data = getattr(nwbfile, group_name)\n",
    "                for name, timeseries in group_data.items():\n",
    "                    data_to_save[name] = np.array(timeseries.data[:])\n",
    "\n",
    "        # Save extracted data to .npz files\n",
    "        for name, data in data_to_save.items():\n",
    "            np.savez_compressed(os.path.join(output_dir, f'{name}.npz'), data=data)\n",
    "\n",
    "        print(\"Data extraction and saving completed.\")\n",
    "\n",
    "def extract_and_save_npz(nwb_file_path, output_dir):\n",
    "    with NWBHDF5IO(nwb_file_path, 'r') as io:\n",
    "        nwbfile = io.read()\n",
    "\n",
    "        # Extract acquisition groups\n",
    "        acquisition_data = {name: np.array(timeseries.data[:]) for name, timeseries in nwbfile.acquisition.items()}\n",
    "\n",
    "        # Extract stimulus groups if present\n",
    "        stimulus_data = {}\n",
    "        if hasattr(nwbfile, 'stimulus'):\n",
    "            for name, timeseries in nwbfile.stimulus.items():\n",
    "                stimulus_data[name] = np.array(timeseries.data[:])\n",
    "\n",
    "        # Extract ElectricalSeries data if present\n",
    "        if 'ElectricalSeries' in nwbfile.acquisition:\n",
    "            electrical_series_data = np.array(nwbfile.acquisition['ElectricalSeries'].data[:])\n",
    "            np.savez_compressed(os.path.join(output_dir, 'ElectricalSeries.npz'), data=electrical_series_data)\n",
    "\n",
    "        # Combine and save the data to .npz files\n",
    "        for name, data in {**acquisition_data, **stimulus_data}.items():\n",
    "            np.savez_compressed(os.path.join(output_dir, f'{name}.npz'), data=data)\n",
    "\n",
    "        print(\"Data extraction and saving completed.\")\n",
    "\n",
    "\n",
    "def load_raw_data(filename: str):\n",
    "    with np.load(filename) as loaded:\n",
    "        # Assuming the .npz file contains an array with the key 'data'\n",
    "        data = loaded['data']\n",
    "    return data\n",
    "\n",
    "def create_array(dirPath:str, offset:int,listFiles:list):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory\n",
    "    offset: [int] time delay before before recording after a stimulus\n",
    "    OUTPUT: \n",
    "    dataset: [np.array] an array of shape (N,8,8,3001) containing the data\n",
    "    \"\"\"\n",
    "    # number of pre and post stimulation files \n",
    "    nbr_files = int(len(listFiles)/2)\n",
    "    total_nbr_stim_per_file,nbr_stim_per_electrode,nbr_electrodes,nbr_neurospheres,seq_len = recording_parameters(dirPath,offset)\n",
    "\n",
    "\n",
    "    # create two arrays of size (N,8,8,3001), each one corresponding to one of the class\n",
    "    raw1_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    raw2_reshaped = np.zeros((total_nbr_stim_per_file*nbr_files,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "\n",
    "    # fill these arrays with corresponding values from files\n",
    "    for start_exp_index in range(nbr_files):\n",
    "        print(f'experiment number: {start_exp_index}')\n",
    "        filename1 = f'{dirPath}/exp_{start_exp_index}_0_{offset}.npz'\n",
    "        filename2 = f'{dirPath}/exp_{start_exp_index}_1_{offset}.npz'\n",
    "\n",
    "        if not os.path.exists(filename1) or not os.path.exists(filename2):\n",
    "            print(f\"Files not found: {filename1} or {filename2}\")\n",
    "            continue  # Skip this iteration if files do not exist\n",
    "\n",
    "        raw1 = load_raw_data(filename1)\n",
    "        raw2 = load_raw_data(filename2)\n",
    "\n",
    "        # iterate through electrode stimulated and neurospheres\n",
    "        for electrode in range(nbr_electrodes):\n",
    "            #nbr_neurospheres = int(raw1[electrode].shape[1]/8)\n",
    "            # N: number of reptition of the stimulus\n",
    "            N = raw1[1].shape[0]\n",
    "            for i in range(nbr_neurospheres):\n",
    "                j = nbr_electrodes*i\n",
    "                raw1_one_file[N*i:N*(i+1),electrode] = raw1[electrode][:N,j:j+nbr_electrodes]\n",
    "                raw2_one_file[N*i:N*(i+1),electrode] = raw2[electrode][:N,j:j+nbr_electrodes]\n",
    "        raw1_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw1_one_file\n",
    "        raw2_reshaped[total_nbr_stim_per_file*start_exp_index:total_nbr_stim_per_file*(start_exp_index+1),:,:,:seq_len] = raw2_one_file\n",
    "\n",
    "    # append label\n",
    "    print(\"append label\")\n",
    "    raw1_reshaped[:,:,:,seq_len] = np.zeros((raw1_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "    raw2_reshaped[:,:,:,seq_len] = np.ones((raw2_reshaped.shape[0],nbr_electrodes,nbr_electrodes))\n",
    "\n",
    "    #return full dataset\n",
    "    print(\"return dataset\")\n",
    "    dataset = np.zeros((total_nbr_stim_per_file*nbr_files*2,nbr_electrodes,nbr_electrodes,seq_len+1))\n",
    "    dataset[:total_nbr_stim_per_file*nbr_files] = raw1_reshaped\n",
    "    dataset[total_nbr_stim_per_file*nbr_files:] = raw2_reshaped\n",
    "    return dataset.astype(np.float32)\n",
    "\n",
    "def recording_parameters(dirPath: str, offset: int):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    dirPath: [str] path to directory containing .npz files\n",
    "    offset: [int] time delay before recording after a stimulus (unused in this function)\n",
    "    OUTPUT:\n",
    "    total_nbr_stim_per_file: [int] total number of stimulations per file\n",
    "    nbr_stim_per_electrode: [int] number of times the experiment is repeated within a file\n",
    "    nbr_electrodes: [int] number of electrodes (usually 8)\n",
    "    nbr_neurospheres: [int] number of neurospheres considered\n",
    "    seq_len: [int] length of the data in the time dimension\n",
    "    \"\"\"\n",
    "\n",
    "    npz_files = [f for f in os.listdir(dirPath) if f.endswith('.npz')]\n",
    "    if not npz_files:\n",
    "        raise FileNotFoundError(\"No .npz files found in the directory\")\n",
    "\n",
    "    first_file = os.path.join(dirPath, npz_files[0])\n",
    "    file_data = load_raw_data(first_file)\n",
    "\n",
    "    if not isinstance(file_data, np.ndarray):\n",
    "        raise ValueError(\"Loaded data is not a numpy array.\")\n",
    "    if file_data.ndim != 2:\n",
    "        raise ValueError(f\"Unexpected number of dimensions in the data: {file_data.ndim}\")\n",
    "\n",
    "    # Assuming first dimension is trials and second dimension is time points\n",
    "    nbr_trials = file_data.shape[0]\n",
    "    time_points = file_data.shape[1]\n",
    "\n",
    "    # Assuming 8 electrodes, 1 neurosphere, and 1 stimulation per electrode\n",
    "    nbr_stim_per_electrode = 1\n",
    "    nbr_electrodes = 8\n",
    "    nbr_neurospheres = 1\n",
    "    total_nbr_stim_per_file = nbr_trials\n",
    "    seq_len = time_points\n",
    "\n",
    "    return total_nbr_stim_per_file, nbr_stim_per_electrode, nbr_electrodes, nbr_neurospheres, seq_len\n",
    "\n",
    "\n",
    "def convertFlatRaw4x3x4x3(raw: np.array):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    raw: [np.array] array of shape (N,1,8,8,M) with N the number of trials and M the length of the sequence\n",
    "    OUTPUT:\n",
    "    array4x3: [np.array] array of shape (N,1,4,3,4,3,M)\n",
    "    \"\"\"\n",
    "    map8 = np.array([[1,0],[0,1],[1,1],[1,2],[2,2],[2,1],[3,1],[2,0]])\n",
    "    array4x3 = torch.zeros((raw.shape[0],1,4,3,4,3,raw.shape[-1]),dtype=torch.float32)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            array4x3[:,0,map8[i,0],map8[i,1],map8[j,0],map8[j,1],:] = raw[:,0,i,j,:]\n",
    "    return array4x3\n",
    "\n",
    "def getListFiles(dirPath):\n",
    "    listFiles = filter(os.path.isfile,glob.glob(f'{dirPath}/*.npz'))\n",
    "    listFiles = sorted(listFiles, key=os.path.getmtime)\n",
    "    return listFiles\n",
    "        \n",
    "def read_in_data(dirPath):\n",
    "    offset_trigger_ms = 5\n",
    "    listFiles = getListFiles(dirPath)\n",
    "    dataset = create_array(dirPath, offset_trigger_ms, listFiles)\n",
    "    print(\"converting spatial position\")\n",
    "    dataset = convertFlatRaw4x3x4x3(dataset)\n",
    "    dataset = np.expand_dims(dataset,1)\n",
    "    return dataset\n",
    "\n",
    "# Define the path to your .nwb file and the output directory for .npz files\n",
    "nwb_file_path = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ/data1.nwb'  # path to your .nwb file\n",
    "output_dir = '/home/vincent/AAA_projects/UnlimitedResearchCooperative/Synthetic Intelligence/human-cortical-organoid-signal-analysis/IntanToNWBtoNPZ'  # path to the directory where you want to save .npz files\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Call the function to extract data from .nwb and save as .npz\n",
    "extract_and_save_npz(nwb_file_path, output_dir)\n",
    "\n",
    "# Now call the read_in_data function with the directory containing your .npz files\n",
    "dataset = read_in_data(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856add5-5a5f-42d8-a928-9ee221563305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8f97-4582-49bf-9aa7-880ebd3e51e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
